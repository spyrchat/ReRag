# Complete Self-Contained Benchmark Scenario Template
# This template shows all required sections for isolated configuration
description: "Template for completely self-contained benchmark scenarios"

# Dataset configuration (REQUIRED)
dataset:
  path: "/home/spiros/Desktop/Thesis/datasets/sosum/data"
  use_ground_truth: true

# Retrieval configuration (REQUIRED)
retrieval:
  type: "hybrid"  # dense, sparse, or hybrid
  top_k: 20
  score_threshold: 0.1

# Embedding configuration (REQUIRED)
embedding:
  dense:
    provider: voyage  # google, voyage, openai, etc.
    model: voyage-3.5-lite
    dimensions: 1024
    api_key_env: VOYAGE_API_KEY
    batch_size: 32
    vector_name: dense
  sparse:
    provider: sparse
    model: Qdrant/bm25
    vector_name: sparse
  strategy: hybrid  # dense, sparse, or hybrid

# Qdrant collection configuration (REQUIRED)
qdrant:
  collection: sosum_stackoverflow_hybrid_v1
  dense_vector_name: dense
  sparse_vector_name: sparse

# Retrievers configuration (REQUIRED by RetrievalPipelineFactory)
retrievers:
  hybrid:
    type: hybrid
    top_k: 20
    score_threshold: 0.1
    dense_weight: 0.6
    sparse_weight: 0.4
    fusion_method: rrf
    fusion:
      method: rrf
      rrf_k: 60
      dense_weight: 0.6
      sparse_weight: 0.4
    performance:
      batch_size: 32
      enable_caching: true
      lazy_initialization: true
      parallel_search: false
    qdrant:
      collection_name: sosum_stackoverflow_hybrid_v1
      dense_vector_name: dense
      sparse_vector_name: sparse
      hybrid_config:
        alpha: 0.5
        reciprocal_rank_constant: 60
    embedding:
      dense:
        provider: voyage
        model: voyage-3.5-lite
        dimensions: 1024
        api_key_env: VOYAGE_API_KEY
        batch_size: 32
      sparse:
        provider: sparse
        model: Qdrant/bm25
      strategy: hybrid

# LLM configuration (OPTIONAL - for generation evaluation)
llm:
  provider: openai
  model: gpt-4o-mini
  temperature: 0.0

# Reranking configuration (OPTIONAL - remove this section for no reranking)
reranking:
  enabled: false  # Set to true to enable reranking
  model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  top_k: 10
  batch_size: 16

# Evaluation configuration (REQUIRED)
evaluation:
  k_values: [1, 5, 10, 20]
  metrics:
    retrieval: ["precision@k", "recall@k", "mrr", "ndcg@k"]

# Experiment parameters (REQUIRED)
max_queries: 50
experiment_name: "template_scenario"

# Generation configuration (OPTIONAL)
generation:
  enabled: false
  provider: openai
  model: gpt-4o-mini
  context_limit: 5

# Benchmark configuration (OPTIONAL - for backwards compatibility)
benchmark:
  evaluation:
    k_values: [1, 5, 10, 20]
    metrics: ["precision", "recall", "f1", "mrr", "ndcg"]
  retrieval:
    strategy: hybrid
    top_k: 20
    score_threshold: 0.1
