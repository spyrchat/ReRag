# === Agent Configuration ===
agent:
  mode: refined  # "simple" (old) or "refined" (new multi-stage pipeline)

# === LLM Configuration ===
# Supported providers: openai, ollama, anthropic, gemini
llm:
  provider: anthropic        # Options: openai, ollama, anthropic, gemini
  model: claude-haiku-4-5    
  temperature: 0.0
  base_url: http://localhost:11434  # Ollama URL (optional, defaults to this)
  
  # Ollama-specific parameters
  num_ctx: 8192              
  num_predict: 2048          

agent_retrieval:
  config_path: pipelines/configs/retrieval/dense_bge_m3.yml

generation:
  prompt_style: conversational  
  # strict:         Strict grounding to context, minimal hallucinations (best for benchmarks)
  # conversational: More natural, allows some inference (best for UX)
  # citations:      Research-focused with explicit references (best for verification)

# === Self-RAG Configuration ===
# Controls iterative refinement with verification
self_rag:
  max_iterations: 5  

# === Benchmark Configuration ===
benchmark:
  enabled: true  # Set to false to disable execution logging
  output_dir: logs/benchmark
