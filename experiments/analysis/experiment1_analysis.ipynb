{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9aa564",
   "metadata": {},
   "source": [
    "# Experiment 1: Retrieval Strategy Baseline Comparison\n",
    "\n",
    "**Analysis & Visualization for Thesis**\n",
    "\n",
    "This notebook provides comprehensive analysis and publication-ready visualizations for Experiment 1, which compares five retrieval strategies:\n",
    "\n",
    "1. **BM25 Baseline** - Traditional sparse retrieval\n",
    "2. **SPLADE Baseline** - Neural sparse retrieval\n",
    "3. **Dense BGE-M3** - Dense semantic retrieval\n",
    "4. **Hybrid SPLADE + BGE-M3** - Hybrid approach\n",
    "5. **Hybrid BM25 + BGE-M3** - Traditional hybrid\n",
    "\n",
    "**Dataset:** StackOverflow (SOSum) - 506 queries\n",
    "\n",
    "**Date:** October 5, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5b003",
   "metadata": {},
   "source": [
    "## Πειραματική Διαδικασία (Experimental Setup)\n",
    "\n",
    "### Δημιουργία Ground Truth\n",
    "\n",
    "Για την αξιολόγηση των συστημάτων ανάκτησης πληροφορίας, δημιουργήθηκε ground truth σε επίπεδο **chunks** (τμημάτων εγγράφων), όχι σε επίπεδο ολόκληρων εγγράφων. Συγκεκριμένα:\n",
    "\n",
    "- **Corpus:** Το σύνολο δεδομένων SOSum (Stack Overflow) περιέχει 506 ερωτήματα (queries)\n",
    "- **Chunking Strategy:** Κάθε έγγραφο (Stack Overflow answer) χωρίζεται σε chunks μεγέθους 512 tokens με overlap 50 tokens\n",
    "- **Ground Truth Mapping:** Για κάθε query, τα relevant chunks προσδιορίζονται με βάση το αρχικό relevant document. Δηλαδή, αν ένα document D είναι relevant για το query Q, τότε **όλα τα chunks** που προέρχονται από το D θεωρούνται relevant για το Q.\n",
    "\n",
    "### Αξιολόγηση σε Επίπεδο Chunks\n",
    "\n",
    "Η αξιολόγηση γίνεται σε επίπεδο chunks, όχι documents:\n",
    "\n",
    "- **Retrieval Results:** Κάθε σύστημα επιστρέφει τα top-k chunks για κάθε query\n",
    "- **Relevance Judgment:** Ένα retrieved chunk θεωρείται σωστό (relevant) αν προέρχεται από ένα document που είναι relevant για το συγκεκριμένο query\n",
    "- **Metrics Calculation:** Οι μετρικές (Precision, Recall, F1, MAP, MRR, NDCG) υπολογίζονται με βάση το πόσα από τα retrieved chunks είναι relevant chunks\n",
    "\n",
    "### Στόχοι Experiment 1: Baseline Comparison\n",
    "\n",
    "Το **Experiment 1** έχει ως στόχο τη **σύγκριση πέντε βασικών στρατηγικών ανάκτησης** (retrieval strategies) χωρίς την εφαρμογή reranking:\n",
    "\n",
    "1. **BM25 Baseline** - Παραδοσιακή λεξικογραφική (sparse) μέθοδος\n",
    "2. **SPLADE Baseline** - Νευρωνική sparse μέθοδος με learned term expansion\n",
    "3. **Dense BGE-M3** - Πυκνή σημασιολογική (dense semantic) ανάκτηση\n",
    "4. **Hybrid SPLADE + BGE-M3** - Υβριδική προσέγγιση (0.5 sparse + 0.5 dense)\n",
    "5. **Hybrid BM25 + BGE-M3** - Παραδοσιακή υβριδική προσέγγιση (0.5 BM25 + 0.5 dense)\n",
    "\n",
    "**Παρατηρήσεις:**\n",
    "- Η αξιολόγηση εστιάζει κυρίως σε **set-based metrics** (Precision, Recall, F1, MAP) λόγω του noise στο ranking του Stack Overflow\n",
    "- Οι rank-aware μετρικές (MRR, NDCG) παρουσιάζονται για σχετική σύγκριση μεταξύ των συστημάτων\n",
    "- Τα αποτελέσματα χρησιμοποιούνται για την επιλογή των καλύτερων retrieval strategies για το **Experiment 2** (reranking optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3d93f",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a401c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ρυθμίσεις matplotlib για ελληνικά (matching llm_judge_plots)\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['DejaVu Serif', 'Times New Roman', 'Liberation Serif']\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Larger font sizes for report readability\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 15\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['figure.titlesize'] = 16\n",
    "\n",
    "# IBM Carbon color palette for the 5 retrieval methods\n",
    "ibm_colors = ['#648FFF', '#785EF0', '#DC267F', '#FE6100', '#FFB000']\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=ibm_colors)\n",
    "\n",
    "print(\"✓ Βιβλιοθήκες φορτώθηκαν επιτυχώς\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51258afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "RESULTS_DIR = Path('../../results/experiment_1')\n",
    "OUTPUT_DIR = Path('../../output/experiment_1_plots')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data files (corrected file patterns)\n",
    "summary_file = list(RESULTS_DIR.glob('experiment_summary_full_*.csv'))[-1]\n",
    "stats_file = list(RESULTS_DIR.glob('experiment_statistical_analysis_full_*.csv'))[-1]\n",
    "full_results_file = list(RESULTS_DIR.glob('experiment_full_results_full_*.json'))[-1]\n",
    "\n",
    "print(f\"Loading data from:\")\n",
    "print(f\"  Summary: {summary_file.name}\")\n",
    "print(f\"  Statistics: {stats_file.name}\")\n",
    "print(f\"  Full Results: {full_results_file.name}\")\n",
    "\n",
    "# Load CSV files\n",
    "df_summary = pd.read_csv(summary_file)\n",
    "df_stats = pd.read_csv(stats_file)\n",
    "\n",
    "# Load JSON file\n",
    "with open(full_results_file, 'r') as f:\n",
    "    full_results = json.load(f)\n",
    "\n",
    "print(\"\\n✅ Data loaded successfully\")\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Scenarios: {len(df_summary)}\")\n",
    "print(f\"  Statistical comparisons: {len(df_stats)}\")\n",
    "print(f\"  Total queries per scenario: {df_summary['total_queries'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb1f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics (including k=10 for Experiment 2 optimization)\n",
    "print(\"\\n📊 Summary Table\\n\")\n",
    "display_cols = ['scenario', 'retrieval_type', 'precision@10_mean', 'recall@10_mean', \n",
    "                'f1@10_mean', 'map_mean', 'mrr_mean', 'ndcg@10_mean', 'time_mean_ms']\n",
    "df_summary[display_cols].round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e50f7c",
   "metadata": {},
   "source": [
    "## 2. Data Preparation & Color Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59bf7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM Carbon Categorical Color Palette (Light theme)\n",
    "# Curated sequence for maximum contrast between neighboring colors\n",
    "CARBON_COLORS = {\n",
    "    'purple_70': '#6929c4',   # 01. Purple 70\n",
    "    'cyan_50': '#1192e8',     # 02. Cyan 50\n",
    "    'teal_70': '#005d5d',     # 03. Teal 70\n",
    "    'magenta_70': '#9f1853',  # 04. Magenta 70\n",
    "    'red_50': '#fa4d56',      # 05. Red 50\n",
    "    'red_90': '#570408',      # 06. Red 90\n",
    "    'green_60': '#198038',    # 07. Green 60\n",
    "    'blue_80': '#002d9c',     # 08. Blue 80\n",
    "    'magenta_50': '#ee538b',  # 09. Magenta 50\n",
    "    'yellow_50': '#b28600',   # 10. Yellow 50\n",
    "    'teal_50': '#009d9a',     # 11. Teal 50\n",
    "    'cyan_90': '#012749',     # 12. Cyan 90\n",
    "    'orange_70': '#8a3800',   # 13. Orange 70\n",
    "    'purple_50': '#a56eff'    # 14. Purple 50\n",
    "}\n",
    "\n",
    "# Apply Carbon colors to retrieval methods (5 distinct methods)\n",
    "# Using sequence order for maximum contrast between neighbors\n",
    "COLOR_SCHEME = {\n",
    "    'BM25_Baseline': CARBON_COLORS['purple_70'],        # 01. Purple - Traditional baseline\n",
    "    'SPLADE_Baseline': CARBON_COLORS['cyan_50'],        # 02. Cyan - Neural sparse  \n",
    "    'Dense_BGE_M3': CARBON_COLORS['green_60'],          # 07. Green - Dense semantic (skipped for contrast)\n",
    "    'Hybrid_SPLADE_BGE_M3': CARBON_COLORS['magenta_70'], # 04. Magenta - Best hybrid\n",
    "    'Hybrid_BM25_BGE_M3': CARBON_COLORS['yellow_50']    # 10. Yellow - Traditional hybrid\n",
    "}\n",
    "\n",
    "# For reference - named colors\n",
    "colors_thesis = {\n",
    "    'primary': CARBON_COLORS['cyan_50'],\n",
    "    'secondary': CARBON_COLORS['purple_70'],\n",
    "    'accent': CARBON_COLORS['magenta_70'],\n",
    "    'success': CARBON_COLORS['green_60'],\n",
    "    'warning': CARBON_COLORS['yellow_50'],\n",
    "    'danger': CARBON_COLORS['red_50'],\n",
    "    'grid': '#e0e0e0',       # Light gray for grid\n",
    "    'text': '#161616'        # Carbon text color\n",
    "}\n",
    "\n",
    "# Readable labels for plots\n",
    "LABEL_MAPPING = {\n",
    "    'BM25_Baseline': 'BM25',\n",
    "    'SPLADE_Baseline': 'SPLADE',\n",
    "    'Dense_BGE_M3': 'Dense\\n(BGE-M3)',\n",
    "    'Hybrid_SPLADE_BGE_M3': 'Hybrid\\n(SPLADE+BGE)',\n",
    "    'Hybrid_BM25_BGE_M3': 'Hybrid\\n(BM25+BGE)'\n",
    "}\n",
    "\n",
    "# Short labels for compact plots\n",
    "SHORT_LABELS = {\n",
    "    'BM25_Baseline': 'BM25',\n",
    "    'SPLADE_Baseline': 'SPLADE',\n",
    "    'Dense_BGE_M3': 'Dense',\n",
    "    'Hybrid_SPLADE_BGE_M3': 'Hybrid-S',\n",
    "    'Hybrid_BM25_BGE_M3': 'Hybrid-B'\n",
    "}\n",
    "\n",
    "# Order for plots\n",
    "SCENARIO_ORDER = ['BM25_Baseline', 'SPLADE_Baseline', 'Dense_BGE_M3', \n",
    "                  'Hybrid_SPLADE_BGE_M3', 'Hybrid_BM25_BGE_M3']\n",
    "\n",
    "print(\"✅ IBM Carbon color palette applied (categorical - maximized contrast)\")\n",
    "print(\"   Colors: Purple → Cyan → Green → Magenta → Yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d064da4",
   "metadata": {},
   "source": [
    "## 3. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(fig, filename, formats=['png', 'pdf']):\n",
    "    \"\"\"Save figure in multiple formats for thesis.\"\"\"\n",
    "    for fmt in formats:\n",
    "        filepath = OUTPUT_DIR / f\"{filename}.{fmt}\"\n",
    "        fig.savefig(filepath, format=fmt, bbox_inches='tight', dpi=300)\n",
    "        print(f\"  ✓ Saved: {filepath}\")\n",
    "\n",
    "def calculate_ci_95(mean, std, n=506):\n",
    "    \"\"\"Calculate 95% confidence interval (±1.96 * SE).\"\"\"\n",
    "    se = std / np.sqrt(n)\n",
    "    ci = 1.96 * se\n",
    "    return ci\n",
    "\n",
    "def format_percentage(value, decimals=1):\n",
    "    \"\"\"Format value as percentage.\"\"\"\n",
    "    return f\"{value*100:.{decimals}f}%\"\n",
    "\n",
    "print(\"✅ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1f02c",
   "metadata": {},
   "source": [
    "## 4. Metric Interpretation Strategy\n",
    "\n",
    "### Understanding Dataset Limitations\n",
    "\n",
    "The **SOSum dataset** uses Stack Overflow's default ranking system (community voting + accepted answers), which doesn't always represent perfect semantic ordering. This affects **rank-aware metrics** (NDCG, MRR) that assume the first answer is most relevant.\n",
    "\n",
    "**Our Approach:**\n",
    "1. **Primary Focus**: Set-based metrics (Precision@k, Recall@k, F1@k, MAP) - less sensitive to ranking noise\n",
    "2. **Secondary Analysis**: Rank-aware metrics (NDCG, MRR) - valuable for *relative comparison* between methods\n",
    "3. **Confidence Intervals**: Use 95% CI instead of raw standard deviation for cleaner visualization\n",
    "\n",
    "**Rationale**: All retrieval methods face the same noisy ground truth, so relative comparisons remain valid. Real-world data with inherent imperfections is preferable to artificial benchmarks.\n",
    "\n",
    "**Note**: Error bars in all plots show 95% confidence intervals (±1.96 × SE), not standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ed6de",
   "metadata": {},
   "source": [
    "## 5. Figure 1: Overall Performance Comparison (MAP, MRR, NDCG@10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b5bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for overall performance metrics\n",
    "metrics_to_plot = ['map_mean', 'mrr_mean', 'ndcg@10_mean']\n",
    "metric_labels = ['MAP', 'MRR', 'NDCG@10']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # Larger size\n",
    "fig.suptitle('Συνολική επίδοση του συστήματος ανάκτησης', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for idx, (metric, label) in enumerate(zip(metrics_to_plot, metric_labels)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Extract data\n",
    "    data = df_summary.set_index('scenario').loc[SCENARIO_ORDER]\n",
    "    values = data[metric].values\n",
    "    std_col = metric.replace('_mean', '_std')\n",
    "    stds = data[std_col].values\n",
    "    # Use 95% CI for cleaner visualization\n",
    "    errors = [calculate_ci_95(v, s) for v, s in zip(values, stds)]\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.bar(range(len(SCENARIO_ORDER)), values, \n",
    "                   color=[COLOR_SCHEME[s] for s in SCENARIO_ORDER],\n",
    "                   yerr=errors, capsize=5, alpha=0.85, edgecolor='black', \n",
    "                   linewidth=1.0, error_kw={'linewidth': 1.5, 'alpha': 0.7})\n",
    "    \n",
    "    # Customize\n",
    "    ax.set_ylabel(label, fontweight='bold', fontsize=14)\n",
    "    ax.set_xticks(range(len(SCENARIO_ORDER)))\n",
    "    ax.set_xticklabels([SHORT_LABELS[s] for s in SCENARIO_ORDER], rotation=45, ha='right', fontsize=12)\n",
    "    ax.set_ylim(0, max(values) * 1.2)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, values)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + errors[i] + 0.01,\n",
    "                f'{val:.3f}',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Highlight best performer\n",
    "    best_idx = np.argmax(values)\n",
    "    bars[best_idx].set_edgecolor('gold')\n",
    "    bars[best_idx].set_linewidth(3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig1_overall_performance')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Figure 1 generated: Overall Performance Comparison\")\n",
    "print(\"   Note: Error bars show 95% confidence intervals (not std)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978ce75",
   "metadata": {},
   "source": [
    "## 6. Figure 2: Precision@k Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4618ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision at different k values (including k=10 for Experiment 2)\n",
    "k_values = [1, 3, 5, 10]\n",
    "precision_metrics = [f'precision@{k}_mean' for k in k_values]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))  # Larger size\n",
    "\n",
    "x = np.arange(len(k_values))\n",
    "width = 0.15\n",
    "\n",
    "for i, scenario in enumerate(SCENARIO_ORDER):\n",
    "    data = df_summary[df_summary['scenario'] == scenario]\n",
    "    values = [data[metric].values[0] for metric in precision_metrics]\n",
    "    # Use 95% CI instead of std for cleaner visualization\n",
    "    stds = [data[metric.replace('_mean', '_std')].values[0] for metric in precision_metrics]\n",
    "    errors = [calculate_ci_95(v, s) for v, s in zip(values, stds)]\n",
    "    \n",
    "    offset = (i - len(SCENARIO_ORDER)/2 + 0.5) * width\n",
    "    bars = ax.bar(x + offset, values, width, label=SHORT_LABELS[scenario],\n",
    "                   color=COLOR_SCHEME[scenario], alpha=0.85, \n",
    "                   yerr=errors, capsize=3, edgecolor='black', linewidth=0.8, \n",
    "                   error_kw={'linewidth': 1.5, 'alpha': 0.7})\n",
    "\n",
    "ax.set_xlabel('k (πλήθος ανακτημένων τμημάτων)', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('Precision@k', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Ακρίβεια για διαφορετικές τιμές αποκοπής', \n",
    "             fontsize=15, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'k={k}' for k in k_values], fontsize=12)\n",
    "ax.legend(loc='upper right', framealpha=0.95, edgecolor='gray', fontsize=12)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig2_precision_at_k')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Figure 2 generated: Precision@k Comparison (with k=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed552b7",
   "metadata": {},
   "source": [
    "## 6. Figure 3: Recall@k Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall at different k values (including k=10)\n",
    "recall_metrics = [f'recall@{k}_mean' for k in k_values]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))  # Larger size\n",
    "\n",
    "x = np.arange(len(k_values))\n",
    "width = 0.15\n",
    "\n",
    "for i, scenario in enumerate(SCENARIO_ORDER):\n",
    "    data = df_summary[df_summary['scenario'] == scenario]\n",
    "    values = [data[metric].values[0] for metric in recall_metrics]\n",
    "    # Use 95% CI instead of std\n",
    "    stds = [data[metric.replace('_mean', '_std')].values[0] for metric in recall_metrics]\n",
    "    errors = [calculate_ci_95(v, s) for v, s in zip(values, stds)]\n",
    "    \n",
    "    offset = (i - len(SCENARIO_ORDER)/2 + 0.5) * width\n",
    "    bars = ax.bar(x + offset, values, width, label=SHORT_LABELS[scenario],\n",
    "                   color=COLOR_SCHEME[scenario], alpha=0.85,\n",
    "                   yerr=errors, capsize=3, edgecolor='black', linewidth=0.8,\n",
    "                   error_kw={'linewidth': 1.5, 'alpha': 0.7})\n",
    "\n",
    "ax.set_xlabel('k (πλήθος ανακτημένων τμημάτων)', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('Recall@k', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Ανάκληση για διαφορετικές τιμές αποκοπής', \n",
    "             fontsize=15, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'k={k}' for k in k_values], fontsize=12)\n",
    "ax.legend(loc='upper left', framealpha=0.95, edgecolor='gray', fontsize=12)\n",
    "ax.set_ylim(0, max([df_summary[metric].max() for metric in recall_metrics]) * 1.15)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig3_recall_at_k')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Figure 3 generated: Recall@k Comparison (with k=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c5466f",
   "metadata": {},
   "source": [
    "## 7. Figure 4: F1@k Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 scores at different k values (including k=10 for consistency)\n",
    "f1_k_values = [3, 5, 10]\n",
    "f1_metrics = [f'f1@{k}_mean' for k in f1_k_values]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))  # Larger size\n",
    "\n",
    "x = np.arange(len(f1_k_values))\n",
    "width = 0.15\n",
    "\n",
    "for i, scenario in enumerate(SCENARIO_ORDER):\n",
    "    data = df_summary[df_summary['scenario'] == scenario]\n",
    "    values = [data[metric].values[0] for metric in f1_metrics]\n",
    "    stds = [data[metric.replace('_mean', '_std')].values[0] for metric in f1_metrics]\n",
    "    errors = [calculate_ci_95(v, s) for v, s in zip(values, stds)]\n",
    "    \n",
    "    offset = (i - len(SCENARIO_ORDER)/2 + 0.5) * width\n",
    "    bars = ax.bar(x + offset, values, width, label=SHORT_LABELS[scenario],\n",
    "                   color=COLOR_SCHEME[scenario], alpha=0.85,\n",
    "                   yerr=errors, capsize=3, edgecolor='black', linewidth=0.8,\n",
    "                   error_kw={'linewidth': 1.5, 'alpha': 0.7})\n",
    "\n",
    "ax.set_xlabel('k (πλήθος ανακτημένων τμημάτων)', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('F1@k', fontweight='bold', fontsize=14)\n",
    "ax.set_title('F1 για διαφορετικές τιμές αποκοπής', \n",
    "             fontsize=15, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'k={k}' for k in f1_k_values], fontsize=12)\n",
    "ax.legend(loc='lower right', framealpha=0.95, edgecolor='gray', fontsize=12)\n",
    "ax.set_ylim(0, max([df_summary[metric].max() for metric in f1_metrics]) * 1.15)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig4_f1_scores')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Figure 4 generated: F1@k Scores (with k=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2687d564",
   "metadata": {},
   "source": [
    "## 8. Figure 5: Precision-Recall Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20008aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall scatter plot at k=5\n",
    "fig, ax = plt.subplots(figsize=(12, 10))  # Larger size\n",
    "\n",
    "for scenario in SCENARIO_ORDER:\n",
    "    data = df_summary[df_summary['scenario'] == scenario]\n",
    "    precision = data['precision@5_mean'].values[0]\n",
    "    recall = data['recall@5_mean'].values[0]\n",
    "    p_std = data['precision@5_std'].values[0]\n",
    "    r_std = data['recall@5_std'].values[0]\n",
    "    \n",
    "    # Use 95% CI for error bars\n",
    "    p_ci = calculate_ci_95(precision, p_std)\n",
    "    r_ci = calculate_ci_95(recall, r_std)\n",
    "    \n",
    "    # Plot point with error bars\n",
    "    ax.errorbar(recall, precision, xerr=r_ci, yerr=p_ci,\n",
    "                marker='o', markersize=16, label=SHORT_LABELS[scenario],\n",
    "                color=COLOR_SCHEME[scenario], capsize=6, capthick=2.5,\n",
    "                linewidth=3, alpha=0.8)\n",
    "\n",
    "# Add quadrant lines\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.4, linewidth=1.5)\n",
    "ax.axvline(x=0.3, color='gray', linestyle='--', alpha=0.4, linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Recall@5', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('Precision@5', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Συμβιβασμός ακρίβειας-ανάκλησης για k = 5\\n', \n",
    "             fontsize=15, fontweight='bold', pad=15)\n",
    "ax.legend(loc='lower left', framealpha=0.95, edgecolor='gray', fontsize=12)\n",
    "ax.grid(True, alpha=0.3, linestyle=':')\n",
    "ax.set_xlim(0, max(df_summary['recall@5_mean']) * 1.15)\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "# Add text annotation for ideal quadrant\n",
    "ax.text(0.45, 0.95, 'Υψηλή ακρίβεια\\nΥψηλή ανάκληση', ha='center', fontsize=11, \n",
    "        style='italic', alpha=0.6, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.3, edgecolor='gray'))\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig5_precision_recall_tradeoff')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Figure 5 generated: Precision-Recall Tradeoff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97290a9d",
   "metadata": {},
   "source": [
    "## 9. Figure 6: NDCG@k Progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a783c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG at different cutoffs (including k=10)\n",
    "ndcg_k_values = [1, 3, 5, 10]\n",
    "ndcg_metrics = [f'ndcg@{k}_mean' for k in ndcg_k_values]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))  # Larger size\n",
    "\n",
    "for scenario in SCENARIO_ORDER:\n",
    "    data = df_summary[df_summary['scenario'] == scenario]\n",
    "    values = [data[metric].values[0] for metric in ndcg_metrics]\n",
    "    \n",
    "    ax.plot(ndcg_k_values, values, marker='o', markersize=12, \n",
    "            linewidth=3.5, label=SHORT_LABELS[scenario],\n",
    "            color=COLOR_SCHEME[scenario], alpha=0.85)\n",
    "\n",
    "ax.set_xlabel('k', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('NDCG@k', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Normalized Discounted Cumulative Gain για διαφορετικές τιμές αποκοπής', \n",
    "             fontsize=15, fontweight='bold', pad=15)\n",
    "ax.set_xticks(ndcg_k_values)\n",
    "ax.set_xticklabels([f'k={k}' for k in ndcg_k_values], fontsize=12)\n",
    "ax.legend(loc='lower left', framealpha=0.95, edgecolor='gray', fontsize=12)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(True, alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig6_ndcg_progression')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Figure 6 generated: NDCG@k Progression (with k=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a445f51",
   "metadata": {},
   "source": [
    "## 10. Figure 7: Query Latency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency comparison (without error bars due to high local variance)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))  # Larger size\n",
    "\n",
    "# Plot 1: Mean latency (no error bars to avoid obscuring)\n",
    "ax = axes[0]\n",
    "data = df_summary.set_index('scenario').loc[SCENARIO_ORDER]\n",
    "means = data['time_mean_ms'].values\n",
    "\n",
    "bars = ax.barh(range(len(SCENARIO_ORDER)), means,\n",
    "               color=[COLOR_SCHEME[s] for s in SCENARIO_ORDER],\n",
    "               alpha=0.85, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "ax.set_yticks(range(len(SCENARIO_ORDER)))\n",
    "ax.set_yticklabels([SHORT_LABELS[s] for s in SCENARIO_ORDER], fontsize=12)\n",
    "ax.set_xlabel('Μέση Καθυστέρηση (ms)', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Σύγκριση Καθυστέρησης Ερωτημάτων\\n(Τοπική εκτέλεση - υψηλή διασπορά)', \n",
    "             fontweight='bold', fontsize=15)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle=':')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, mean) in enumerate(zip(bars, means)):\n",
    "    ax.text(mean + 20, bar.get_y() + bar.get_height()/2,\n",
    "            f'{mean:.1f} ms',\n",
    "            va='center', ha='left', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 2: Latency percentiles (cleaner visualization)\n",
    "ax = axes[1]\n",
    "metrics_latency = ['time_median_ms', 'time_p95_ms']\n",
    "x = np.arange(len(SCENARIO_ORDER))\n",
    "width = 0.35\n",
    "\n",
    "median_vals = data['time_median_ms'].values\n",
    "p95_vals = data['time_p95_ms'].values\n",
    "\n",
    "bars1 = ax.bar(x - width/2, median_vals, width, label='Διάμεσος (P50)',\n",
    "               color=COLORS[0], alpha=0.85, \n",
    "               edgecolor='black', linewidth=1)\n",
    "bars2 = ax.bar(x + width/2, p95_vals, width, label='P95',\n",
    "               color=COLORS[3], alpha=0.85, \n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "ax.set_ylabel('Καθυστέρηση (ms)', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Εκατοστημόρια Καθυστέρησης', fontweight='bold', fontsize=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([SHORT_LABELS[s] for s in SCENARIO_ORDER], \n",
    "                   rotation=45, ha='right', fontsize=11)\n",
    "ax.legend(framealpha=0.95, edgecolor='gray', fontsize=12)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig7_latency_analysis')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Figure 7 generated: Query Latency Analysis (cleaned up)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448d43c",
   "metadata": {},
   "source": [
    "## 11. Figure 8: Statistical Significance Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4988a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairwise comparison matrix for MAP metric\n",
    "map_stats = df_stats[df_stats['metric'] == 'map'].copy()\n",
    "\n",
    "# Create matrix\n",
    "scenarios = SCENARIO_ORDER\n",
    "n = len(scenarios)\n",
    "significance_matrix = np.zeros((n, n))\n",
    "effect_size_matrix = np.zeros((n, n))\n",
    "\n",
    "for _, row in map_stats.iterrows():\n",
    "    if row['method1'] in scenarios and row['method2'] in scenarios:\n",
    "        i = scenarios.index(row['method1'])\n",
    "        j = scenarios.index(row['method2'])\n",
    "        \n",
    "        # Significance (1 = significant, 0 = not significant)\n",
    "        significance_matrix[i, j] = 1 if row['bonferroni_significant'] else 0\n",
    "        significance_matrix[j, i] = 1 if row['bonferroni_significant'] else 0  # Symmetric\n",
    "        \n",
    "        # Effect size (symmetric but with opposite sign)\n",
    "        if pd.notna(row['effect_size']):\n",
    "            effect_size_matrix[i, j] = row['effect_size']\n",
    "            effect_size_matrix[j, i] = -row['effect_size']  # Opposite sign for symmetric position\n",
    "\n",
    "# Create improved heatmap with better styling\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 9))  # Larger size\n",
    "fig.suptitle('Statistical Significance Analysis - MAP Metric', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "# --- LEFT: Significance heatmap with cleaner design ---\n",
    "ax = axes[0]\n",
    "\n",
    "# Use discrete colormap for binary significance\n",
    "from matplotlib.colors import ListedColormap\n",
    "sig_colors = ['#fee5d9', '#a50f15']  # Light red (non-sig) to dark red (sig)\n",
    "sig_cmap = ListedColormap(sig_colors)\n",
    "\n",
    "im1 = ax.imshow(significance_matrix, cmap=sig_cmap, aspect='auto', vmin=0, vmax=1, \n",
    "                interpolation='nearest', alpha=0.9)\n",
    "\n",
    "# Add gridlines for clarity\n",
    "for i in range(n+1):\n",
    "    ax.axhline(i-0.5, color='white', linewidth=2.5)\n",
    "    ax.axvline(i-0.5, color='white', linewidth=2.5)\n",
    "\n",
    "ax.set_xticks(np.arange(n))\n",
    "ax.set_yticks(np.arange(n))\n",
    "ax.set_xticklabels([SHORT_LABELS[s] for s in scenarios], rotation=45, ha='right', fontsize=13)\n",
    "ax.set_yticklabels([SHORT_LABELS[s] for s in scenarios], fontsize=13)\n",
    "ax.set_title('Statistical Significance\\n(Bonferroni Corrected p < 0.05)', \n",
    "             fontweight='bold', pad=20, fontsize=15)\n",
    "\n",
    "# Add clean annotations with stars for significance\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i == j:\n",
    "            # Diagonal - same method\n",
    "            ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, \n",
    "                                       fill=True, facecolor='lightgray', \n",
    "                                       edgecolor='white', linewidth=2.5, alpha=0.5))\n",
    "            ax.text(j, i, '—', ha=\"center\", va=\"center\", \n",
    "                   color='gray', fontsize=20, fontweight='bold')\n",
    "        else:\n",
    "            # Off-diagonal - show significance\n",
    "            if significance_matrix[i, j] == 1:\n",
    "                ax.text(j, i, '***', ha=\"center\", va=\"center\", \n",
    "                       color='white', fontsize=24, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(j, i, 'n.s.', ha=\"center\", va=\"center\", \n",
    "                       color='#666', fontsize=11, style='italic', alpha=0.7)\n",
    "\n",
    "# Custom colorbar\n",
    "cbar1 = plt.colorbar(im1, ax=ax, ticks=[0.25, 0.75], pad=0.02)\n",
    "cbar1.ax.set_yticklabels(['Not Significant', 'Significant'], fontsize=11)\n",
    "cbar1.ax.tick_params(size=0)\n",
    "\n",
    "# --- RIGHT: Effect size heatmap with improved styling ---\n",
    "ax = axes[1]\n",
    "\n",
    "# Use diverging colormap centered at 0\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "norm = TwoSlopeNorm(vmin=-1.5, vcenter=0, vmax=1.5)\n",
    "im2 = ax.imshow(effect_size_matrix, cmap='RdBu_r', aspect='auto', norm=norm, \n",
    "                interpolation='nearest', alpha=0.9)\n",
    "\n",
    "# Add gridlines\n",
    "for i in range(n+1):\n",
    "    ax.axhline(i-0.5, color='white', linewidth=2.5)\n",
    "    ax.axvline(i-0.5, color='white', linewidth=2.5)\n",
    "\n",
    "ax.set_xticks(np.arange(n))\n",
    "ax.set_yticks(np.arange(n))\n",
    "ax.set_xticklabels([SHORT_LABELS[s] for s in scenarios], rotation=45, ha='right', fontsize=13)\n",
    "ax.set_yticklabels([SHORT_LABELS[s] for s in scenarios], fontsize=13)\n",
    "ax.set_title('Effect Size (Cohen\\'s d)\\nPositive = Row > Column', \n",
    "             fontweight='bold', pad=20, fontsize=15)\n",
    "\n",
    "# Add clean annotations with better visibility\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i == j:\n",
    "            # Diagonal\n",
    "            ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, \n",
    "                                       fill=True, facecolor='lightgray', \n",
    "                                       edgecolor='white', linewidth=2.5, alpha=0.5))\n",
    "            ax.text(j, i, '—', ha=\"center\", va=\"center\", \n",
    "                   color='gray', fontsize=20, fontweight='bold')\n",
    "        elif effect_size_matrix[i, j] != 0:\n",
    "            # Show effect size with color-adaptive text\n",
    "            value = effect_size_matrix[i, j]\n",
    "            # Choose text color based on background intensity\n",
    "            text_color = 'white' if abs(value) > 0.5 else 'black'\n",
    "            \n",
    "            ax.text(j, i, f'{value:.2f}', ha=\"center\", va=\"center\", \n",
    "                   color=text_color, fontsize=13, fontweight='bold')\n",
    "            \n",
    "            # Add effect size interpretation as subscript\n",
    "            if abs(value) >= 0.8:\n",
    "                size_label = 'L'  # Large\n",
    "            elif abs(value) >= 0.5:\n",
    "                size_label = 'M'  # Medium\n",
    "            elif abs(value) >= 0.2:\n",
    "                size_label = 'S'  # Small\n",
    "            else:\n",
    "                size_label = ''\n",
    "            \n",
    "            if size_label:\n",
    "                ax.text(j, i+0.35, size_label, ha=\"center\", va=\"center\", \n",
    "                       color=text_color, fontsize=9, style='italic', alpha=0.8)\n",
    "\n",
    "# Custom colorbar with interpretations\n",
    "cbar2 = plt.colorbar(im2, ax=ax, pad=0.02)\n",
    "cbar2.set_label('Effect Size', fontsize=12, fontweight='bold')\n",
    "cbar2.ax.tick_params(labelsize=11)\n",
    "\n",
    "# Add effect size interpretation legend\n",
    "legend_text = 'S: Small (0.2)\\nM: Medium (0.5)\\nL: Large (0.8)'\n",
    "ax.text(1.25, -0.15, legend_text, transform=ax.transAxes,\n",
    "        fontsize=10, verticalalignment='top', style='italic',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.3, edgecolor='gray'))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "save_figure(fig, 'fig8_statistical_significance')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Figure 8 generated: Statistical Significance Analysis (improved design)\")\n",
    "print(\"   - Stars (***) indicate statistical significance (p < 0.05)\")\n",
    "print(\"   - Effect sizes labeled with magnitude (S/M/L)\")\n",
    "print(\"   - n.s. = not significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341f2c7",
   "metadata": {},
   "source": [
    "## 12. Figure 9: Comprehensive Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f40cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary figure\n",
    "fig = plt.figure(figsize=(22, 12))  # Much larger size\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.35)\n",
    "\n",
    "# 1. MAP comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "data = df_summary.set_index('scenario').loc[SCENARIO_ORDER]\n",
    "bars = ax1.bar(range(len(SCENARIO_ORDER)), data['map_mean'].values,\n",
    "               color=[COLOR_SCHEME[s] for s in SCENARIO_ORDER], alpha=0.85,\n",
    "               edgecolor='black', linewidth=1.2)\n",
    "ax1.set_title('MAP', fontweight='bold', fontsize=14)\n",
    "ax1.set_xticks(range(len(SCENARIO_ORDER)))\n",
    "ax1.set_xticklabels([SHORT_LABELS[s] for s in SCENARIO_ORDER], rotation=45, ha='right', fontsize=11)\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "ax1.tick_params(axis='y', labelsize=11)\n",
    "# Highlight best\n",
    "best_idx = np.argmax(data['map_mean'].values)\n",
    "bars[best_idx].set_edgecolor('gold')\n",
    "bars[best_idx].set_linewidth(3.5)\n",
    "\n",
    "# 2. MRR comparison\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "bars = ax2.bar(range(len(SCENARIO_ORDER)), data['mrr_mean'].values,\n",
    "               color=[COLOR_SCHEME[s] for s in SCENARIO_ORDER], alpha=0.85,\n",
    "               edgecolor='black', linewidth=1.2)\n",
    "ax2.set_title('MRR*', fontweight='bold', fontsize=14)\n",
    "ax2.set_xticks(range(len(SCENARIO_ORDER)))\n",
    "ax2.set_xticklabels([SHORT_LABELS[s] for s in SCENARIO_ORDER], rotation=45, ha='right', fontsize=11)\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "ax2.tick_params(axis='y', labelsize=11)\n",
    "best_idx = np.argmax(data['mrr_mean'].values)\n",
    "bars[best_idx].set_edgecolor('gold')\n",
    "bars[best_idx].set_linewidth(3.5)\n",
    "\n",
    "# 3. NDCG@10 comparison\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "bars = ax3.bar(range(len(SCENARIO_ORDER)), data['ndcg@10_mean'].values,\n",
    "               color=[COLOR_SCHEME[s] for s in SCENARIO_ORDER], alpha=0.85,\n",
    "               edgecolor='black', linewidth=1.2)\n",
    "ax3.set_title('NDCG@10*', fontweight='bold', fontsize=14)\n",
    "ax3.set_xticks(range(len(SCENARIO_ORDER)))\n",
    "ax3.set_xticklabels([SHORT_LABELS[s] for s in SCENARIO_ORDER], rotation=45, ha='right', fontsize=11)\n",
    "ax3.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "ax3.tick_params(axis='y', labelsize=11)\n",
    "best_idx = np.argmax(data['ndcg@10_mean'].values)\n",
    "bars[best_idx].set_edgecolor('gold')\n",
    "bars[best_idx].set_linewidth(3.5)\n",
    "\n",
    "# 4. Precision@k curves (with k=10)\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "k_vals = [1, 3, 5, 10]\n",
    "for scenario in SCENARIO_ORDER:\n",
    "    scenario_data = df_summary[df_summary['scenario'] == scenario]\n",
    "    values = [scenario_data[f'precision@{k}_mean'].values[0] for k in k_vals]\n",
    "    ax4.plot(k_vals, values, marker='o', markersize=13, \n",
    "            linewidth=3.5, label=SHORT_LABELS[scenario],\n",
    "            color=COLOR_SCHEME[scenario], alpha=0.85)\n",
    "ax4.set_xlabel('k', fontweight='bold', fontsize=14)\n",
    "ax4.set_ylabel('Precision@k', fontweight='bold', fontsize=14)\n",
    "ax4.set_title('Precision at Different Cutoffs', fontweight='bold', fontsize=15)\n",
    "ax4.legend(loc='upper right', ncol=5, framealpha=0.95, fontsize=12, edgecolor='gray')\n",
    "ax4.grid(True, alpha=0.3, linestyle=':')\n",
    "ax4.set_xticks(k_vals)\n",
    "ax4.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "# 5. Recall@k curves (with k=10)\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "for scenario in SCENARIO_ORDER:\n",
    "    scenario_data = df_summary[df_summary['scenario'] == scenario]\n",
    "    values = [scenario_data[f'recall@{k}_mean'].values[0] for k in k_vals]\n",
    "    ax5.plot(k_vals, values, marker='s', markersize=13, \n",
    "            linewidth=3.5, label=SHORT_LABELS[scenario],\n",
    "            color=COLOR_SCHEME[scenario], alpha=0.85)\n",
    "ax5.set_xlabel('k', fontweight='bold', fontsize=14)\n",
    "ax5.set_ylabel('Recall@k', fontweight='bold', fontsize=14)\n",
    "ax5.set_title('Recall at Different Cutoffs', fontweight='bold', fontsize=15)\n",
    "ax5.legend(loc='lower right', ncol=5, framealpha=0.95, fontsize=12, edgecolor='gray')\n",
    "ax5.grid(True, alpha=0.3, linestyle=':')\n",
    "ax5.set_xticks(k_vals)\n",
    "ax5.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "fig.suptitle('Experiment 1: Retrieval Strategy Comprehensive Comparison\\n(*Rank-aware metrics)', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "save_figure(fig, 'fig9_comprehensive_dashboard')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Figure 9 generated: Comprehensive Summary Dashboard (updated with k=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431019da",
   "metadata": {},
   "source": [
    "## 13. Summary Table for Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159277fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication-ready summary table\n",
    "summary_table = df_summary[['scenario', 'precision@5_mean', 'precision@5_std',\n",
    "                             'recall@5_mean', 'recall@5_std',\n",
    "                             'f1@5_mean', 'f1@5_std',\n",
    "                             'map_mean', 'map_std',\n",
    "                             'mrr_mean', 'mrr_std',\n",
    "                             'ndcg@5_mean', 'ndcg@5_std',\n",
    "                             'time_mean_ms', 'time_std_ms']].copy()\n",
    "\n",
    "# Rename columns for readability\n",
    "summary_table['scenario'] = summary_table['scenario'].map(SHORT_LABELS)\n",
    "summary_table.columns = ['Method', 'P@5', 'P@5_std', 'R@5', 'R@5_std', \n",
    "                         'F1@5', 'F1@5_std', 'MAP', 'MAP_std',\n",
    "                         'MRR', 'MRR_std', 'NDCG@5', 'NDCG@5_std',\n",
    "                         'Latency(ms)', 'Lat_std']\n",
    "\n",
    "# Format for display\n",
    "display_table = summary_table.copy()\n",
    "for col in display_table.columns[1:]:\n",
    "    if 'std' in col:\n",
    "        display_table[col] = display_table[col].apply(lambda x: f'±{x:.3f}')\n",
    "    elif 'Latency' in col:\n",
    "        display_table[col] = display_table[col].apply(lambda x: f'{x:.1f}')\n",
    "    else:\n",
    "        display_table[col] = display_table[col].apply(lambda x: f'{x:.3f}')\n",
    "\n",
    "# Combine mean and std\n",
    "result_table = pd.DataFrame({\n",
    "    'Method': display_table['Method'],\n",
    "    'Precision@5': display_table['P@5'] + ' ' + display_table['P@5_std'],\n",
    "    'Recall@5': display_table['R@5'] + ' ' + display_table['R@5_std'],\n",
    "    'F1@5': display_table['F1@5'] + ' ' + display_table['F1@5_std'],\n",
    "    'MAP': display_table['MAP'] + ' ' + display_table['MAP_std'],\n",
    "    'MRR': display_table['MRR'] + ' ' + display_table['MRR_std'],\n",
    "    'NDCG@5': display_table['NDCG@5'] + ' ' + display_table['NDCG@5_std'],\n",
    "    'Latency(ms)': display_table['Latency(ms)'] + ' ' + display_table['Lat_std']\n",
    "})\n",
    "\n",
    "print(\"\\n📊 Summary Table for Thesis (Mean ± Std)\\n\")\n",
    "print(result_table.to_string(index=False))\n",
    "\n",
    "# Save as CSV and LaTeX\n",
    "result_table.to_csv(OUTPUT_DIR / 'table1_summary_results.csv', index=False)\n",
    "result_table.to_latex(OUTPUT_DIR / 'table1_summary_results.tex', index=False, escape=False)\n",
    "\n",
    "print(\"\\n✅ Summary table saved as CSV and LaTeX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0bdb54",
   "metadata": {},
   "source": [
    "## 14. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71872bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key findings\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS FROM EXPERIMENT 1\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Best performers\n",
    "best_map = df_summary.loc[df_summary['map_mean'].idxmax()]\n",
    "print(f\"🏆 BEST OVERALL (MAP): {SHORT_LABELS[best_map['scenario']]}\")\n",
    "print(f\"   MAP = {best_map['map_mean']:.4f} ± {best_map['map_std']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Compare to baseline\n",
    "baseline = df_summary[df_summary['scenario'] == 'BM25_Baseline'].iloc[0]\n",
    "best_improvement = ((best_map['map_mean'] - baseline['map_mean']) / baseline['map_mean']) * 100\n",
    "print(f\"📈 IMPROVEMENT OVER BM25 BASELINE: {best_improvement:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Precision leader\n",
    "best_precision = df_summary.loc[df_summary['precision@5_mean'].idxmax()]\n",
    "print(f\"🎯 HIGHEST PRECISION@5: {SHORT_LABELS[best_precision['scenario']]}\")\n",
    "print(f\"   P@5 = {best_precision['precision@5_mean']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Recall leader\n",
    "best_recall = df_summary.loc[df_summary['recall@5_mean'].idxmax()]\n",
    "print(f\"🔍 HIGHEST RECALL@5: {SHORT_LABELS[best_recall['scenario']]}\")\n",
    "print(f\"   R@5 = {best_recall['recall@5_mean']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Fastest\n",
    "fastest = df_summary.loc[df_summary['time_mean_ms'].idxmin()]\n",
    "print(f\"⚡ FASTEST: {SHORT_LABELS[fastest['scenario']]}\")\n",
    "print(f\"   Latency = {fastest['time_mean_ms']:.1f} ms\")\n",
    "print()\n",
    "\n",
    "# Statistical significance summary\n",
    "sig_comparisons = df_stats[df_stats['bonferroni_significant'] == True]\n",
    "print(f\"📊 STATISTICAL ANALYSIS:\")\n",
    "print(f\"   Total pairwise comparisons: {len(df_stats)}\")\n",
    "print(f\"   Statistically significant: {len(sig_comparisons)}\")\n",
    "print(f\"   Significance rate: {len(sig_comparisons)/len(df_stats)*100:.1f}%\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save findings to text file\n",
    "with open(OUTPUT_DIR / 'key_findings.txt', 'w') as f:\n",
    "    f.write(\"KEY FINDINGS FROM EXPERIMENT 1\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Best Overall (MAP): {SHORT_LABELS[best_map['scenario']]}\\n\")\n",
    "    f.write(f\"MAP = {best_map['map_mean']:.4f} ± {best_map['map_std']:.4f}\\n\\n\")\n",
    "    f.write(f\"Improvement over BM25: {best_improvement:.1f}%\\n\\n\")\n",
    "    f.write(f\"Highest Precision@5: {SHORT_LABELS[best_precision['scenario']]} = {best_precision['precision@5_mean']:.4f}\\n\")\n",
    "    f.write(f\"Highest Recall@5: {SHORT_LABELS[best_recall['scenario']]} = {best_recall['recall@5_mean']:.4f}\\n\")\n",
    "    f.write(f\"Fastest: {SHORT_LABELS[fastest['scenario']]} = {fastest['time_mean_ms']:.1f} ms\\n\")\n",
    "\n",
    "print(\"\\n✅ Key findings saved to key_findings.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fffbe8d",
   "metadata": {},
   "source": [
    "## 15. Export All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPORT SUMMARY\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(f\"📁 Output Directory: {OUTPUT_DIR}\\n\")\n",
    "print(\"Generated Files:\")\n",
    "print(\"  Figures (9 total):\")\n",
    "print(\"    ✓ fig1_overall_performance.png/.pdf\")\n",
    "print(\"    ✓ fig2_precision_at_k.png/.pdf\")\n",
    "print(\"    ✓ fig3_recall_at_k.png/.pdf\")\n",
    "print(\"    ✓ fig4_f1_scores.png/.pdf\")\n",
    "print(\"    ✓ fig5_precision_recall_tradeoff.png/.pdf\")\n",
    "print(\"    ✓ fig6_ndcg_progression.png/.pdf\")\n",
    "print(\"    ✓ fig7_latency_analysis.png/.pdf\")\n",
    "print(\"    ✓ fig8_statistical_significance.png/.pdf\")\n",
    "print(\"    ✓ fig9_comprehensive_dashboard.png/.pdf\")\n",
    "print(\"\\n  Tables:\")\n",
    "print(\"    ✓ table1_summary_results.csv\")\n",
    "print(\"    ✓ table1_summary_results.tex\")\n",
    "print(\"\\n  Analysis:\")\n",
    "print(\"    ✓ key_findings.txt\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n✅ All visualizations and tables generated successfully!\")\n",
    "print(\"\\n📝 Ready for thesis inclusion\")\n",
    "print(f\"\\n🎓 Use these figures in your thesis Chapter 4 (Experimental Evaluation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87999cb8",
   "metadata": {},
   "source": [
    "## Notes for Thesis\n",
    "\n",
    "### Recommended Figure Placement\n",
    "\n",
    "1. **Section 4.1.4.1 - Overall Performance**\n",
    "   - Figure 1: Overall Performance Comparison (MAP, MRR, NDCG@5)\n",
    "   - Table 1: Summary Results\n",
    "\n",
    "2. **Section 4.1.4.2 - Precision & Recall Analysis**\n",
    "   - Figure 2: Precision@k Comparison\n",
    "   - Figure 3: Recall@k Comparison\n",
    "   - Figure 5: Precision-Recall Tradeoff\n",
    "\n",
    "3. **Section 4.1.4.3 - Ranking Quality**\n",
    "   - Figure 6: NDCG@k Progression\n",
    "   - Figure 4: F1@k Scores\n",
    "\n",
    "4. **Section 4.1.4.4 - Computational Efficiency**\n",
    "   - Figure 7: Latency Analysis\n",
    "\n",
    "5. **Section 4.1.4.5 - Statistical Validation**\n",
    "   - Figure 8: Statistical Significance Heatmap\n",
    "\n",
    "6. **Appendix or Summary Section**\n",
    "   - Figure 9: Comprehensive Dashboard\n",
    "\n",
    "### Key Points to Highlight\n",
    "\n",
    "1. Dense BGE-M3 achieves highest MAP (0.546) - 108% improvement over BM25\n",
    "2. SPLADE significantly outperforms traditional BM25 (79% improvement)\n",
    "3. Hybrid methods show varying effectiveness depending on component combination\n",
    "4. All improvements are statistically significant (p < 0.001, large effect sizes)\n",
    "5. Trade-off between latency and quality exists (dense methods are slower)\n",
    "\n",
    "### LaTeX Integration\n",
    "\n",
    "```latex\n",
    "\\begin{figure}[htbp]\n",
    "    \\centering\n",
    "    \\includegraphics[width=0.9\\textwidth]{figures/fig1_overall_performance.pdf}\n",
    "    \\caption{Overall retrieval quality comparison across five methods. Dense BGE-M3 achieves \n",
    "    the highest performance across all metrics (MAP=0.546, MRR=0.928, NDCG@5=0.744), \n",
    "    representing a 108\\% improvement over the BM25 baseline. Error bars show ±1 standard deviation.}\n",
    "    \\label{fig:exp1_overall}\n",
    "\\end{figure}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
