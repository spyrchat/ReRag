{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9aa564",
   "metadata": {},
   "source": [
    "# Experiment 1: Retrieval Strategy Baseline Comparison\n",
    "\n",
    "**Analysis & Visualization for Thesis**\n",
    "\n",
    "This notebook provides comprehensive analysis and publication-ready visualizations for Experiment 1, which compares five retrieval strategies:\n",
    "\n",
    "1. **BM25 Baseline** - Traditional sparse retrieval\n",
    "2. **SPLADE Baseline** - Neural sparse retrieval\n",
    "3. **Dense BGE-M3** - Dense semantic retrieval\n",
    "4. **Hybrid SPLADE + BGE-M3** - Hybrid approach\n",
    "5. **Hybrid BM25 + BGE-M3** - Traditional hybrid\n",
    "\n",
    "**Dataset:** StackOverflow (SOSum) - 506 queries\n",
    "\n",
    "**Date:** October 5, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5b003",
   "metadata": {},
   "source": [
    "## Î ÎµÎ¹ÏÎ±Î¼Î±Ï„Î¹ÎºÎ® Î”Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± (Experimental Setup)\n",
    "\n",
    "### Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ground Truth\n",
    "\n",
    "Î“Î¹Î± Ï„Î·Î½ Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Ï„Ï‰Î½ ÏƒÏ…ÏƒÏ„Î·Î¼Î¬Ï„Ï‰Î½ Î±Î½Î¬ÎºÏ„Î·ÏƒÎ·Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î±Ï‚, Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎµ ground truth ÏƒÎµ ÎµÏ€Î¯Ï€ÎµÎ´Î¿ **chunks** (Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½ ÎµÎ³Î³ÏÎ¬Ï†Ï‰Î½), ÏŒÏ‡Î¹ ÏƒÎµ ÎµÏ€Î¯Ï€ÎµÎ´Î¿ Î¿Î»ÏŒÎºÎ»Î·ÏÏ‰Î½ ÎµÎ³Î³ÏÎ¬Ï†Ï‰Î½. Î£Ï…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î±:\n",
    "\n",
    "- **Corpus:** Î¤Î¿ ÏƒÏÎ½Î¿Î»Î¿ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ SOSum (Stack Overflow) Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ 506 ÎµÏÏ‰Ï„Î®Î¼Î±Ï„Î± (queries)\n",
    "- **Chunking Strategy:** ÎšÎ¬Î¸Îµ Î­Î³Î³ÏÎ±Ï†Î¿ (Stack Overflow answer) Ï‡Ï‰ÏÎ¯Î¶ÎµÏ„Î±Î¹ ÏƒÎµ chunks Î¼ÎµÎ³Î­Î¸Î¿Ï…Ï‚ 512 tokens Î¼Îµ overlap 50 tokens\n",
    "- **Ground Truth Mapping:** Î“Î¹Î± ÎºÎ¬Î¸Îµ query, Ï„Î± relevant chunks Ï€ÏÎ¿ÏƒÎ´Î¹Î¿ÏÎ¯Î¶Î¿Î½Ï„Î±Î¹ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿ Î±ÏÏ‡Î¹ÎºÏŒ relevant document. Î”Î·Î»Î±Î´Î®, Î±Î½ Î­Î½Î± document D ÎµÎ¯Î½Î±Î¹ relevant Î³Î¹Î± Ï„Î¿ query Q, Ï„ÏŒÏ„Îµ **ÏŒÎ»Î± Ï„Î± chunks** Ï€Î¿Ï… Ï€ÏÎ¿Î­ÏÏ‡Î¿Î½Ï„Î±Î¹ Î±Ï€ÏŒ Ï„Î¿ D Î¸ÎµÏ‰ÏÎ¿ÏÎ½Ï„Î±Î¹ relevant Î³Î¹Î± Ï„Î¿ Q.\n",
    "\n",
    "### Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÏƒÎµ Î•Ï€Î¯Ï€ÎµÎ´Î¿ Chunks\n",
    "\n",
    "Î— Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Î³Î¯Î½ÎµÏ„Î±Î¹ ÏƒÎµ ÎµÏ€Î¯Ï€ÎµÎ´Î¿ chunks, ÏŒÏ‡Î¹ documents:\n",
    "\n",
    "- **Retrieval Results:** ÎšÎ¬Î¸Îµ ÏƒÏÏƒÏ„Î·Î¼Î± ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î± top-k chunks Î³Î¹Î± ÎºÎ¬Î¸Îµ query\n",
    "- **Relevance Judgment:** ÎˆÎ½Î± retrieved chunk Î¸ÎµÏ‰ÏÎµÎ¯Ï„Î±Î¹ ÏƒÏ‰ÏƒÏ„ÏŒ (relevant) Î±Î½ Ï€ÏÎ¿Î­ÏÏ‡ÎµÏ„Î±Î¹ Î±Ï€ÏŒ Î­Î½Î± document Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ relevant Î³Î¹Î± Ï„Î¿ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿ query\n",
    "- **Metrics Calculation:** ÎŸÎ¹ Î¼ÎµÏ„ÏÎ¹ÎºÎ­Ï‚ (Precision, Recall, F1, MAP, MRR, NDCG) Ï…Ï€Î¿Î»Î¿Î³Î¯Î¶Î¿Î½Ï„Î±Î¹ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿ Ï€ÏŒÏƒÎ± Î±Ï€ÏŒ Ï„Î± retrieved chunks ÎµÎ¯Î½Î±Î¹ relevant chunks\n",
    "\n",
    "### Î£Ï„ÏŒÏ‡Î¿Î¹ Experiment 1: Baseline Comparison\n",
    "\n",
    "Î¤Î¿ **Experiment 1** Î­Ï‡ÎµÎ¹ Ï‰Ï‚ ÏƒÏ„ÏŒÏ‡Î¿ Ï„Î· **ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ· Ï€Î­Î½Ï„Îµ Î²Î±ÏƒÎ¹ÎºÏÎ½ ÏƒÏ„ÏÎ±Ï„Î·Î³Î¹ÎºÏÎ½ Î±Î½Î¬ÎºÏ„Î·ÏƒÎ·Ï‚** (retrieval strategies) Ï‡Ï‰ÏÎ¯Ï‚ Ï„Î·Î½ ÎµÏ†Î±ÏÎ¼Î¿Î³Î® reranking:\n",
    "\n",
    "1. **BM25 Baseline** - Î Î±ÏÎ±Î´Î¿ÏƒÎ¹Î±ÎºÎ® Î»ÎµÎ¾Î¹ÎºÎ¿Î³ÏÎ±Ï†Î¹ÎºÎ® (sparse) Î¼Î­Î¸Î¿Î´Î¿Ï‚\n",
    "2. **SPLADE Baseline** - ÎÎµÏ…ÏÏ‰Î½Î¹ÎºÎ® sparse Î¼Î­Î¸Î¿Î´Î¿Ï‚ Î¼Îµ learned term expansion\n",
    "3. **Dense BGE-M3** - Î Ï…ÎºÎ½Î® ÏƒÎ·Î¼Î±ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÎ® (dense semantic) Î±Î½Î¬ÎºÏ„Î·ÏƒÎ·\n",
    "4. **Hybrid SPLADE + BGE-M3** - Î¥Î²ÏÎ¹Î´Î¹ÎºÎ® Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ· (0.5 sparse + 0.5 dense)\n",
    "5. **Hybrid BM25 + BGE-M3** - Î Î±ÏÎ±Î´Î¿ÏƒÎ¹Î±ÎºÎ® Ï…Î²ÏÎ¹Î´Î¹ÎºÎ® Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ· (0.5 BM25 + 0.5 dense)\n",
    "\n",
    "**Î Î±ÏÎ±Ï„Î·ÏÎ®ÏƒÎµÎ¹Ï‚:**\n",
    "- Î— Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÎµÏƒÏ„Î¹Î¬Î¶ÎµÎ¹ ÎºÏ…ÏÎ¯Ï‰Ï‚ ÏƒÎµ **set-based metrics** (Precision, Recall, F1, MAP) Î»ÏŒÎ³Ï‰ Ï„Î¿Ï… noise ÏƒÏ„Î¿ ranking Ï„Î¿Ï… Stack Overflow\n",
    "- ÎŸÎ¹ rank-aware Î¼ÎµÏ„ÏÎ¹ÎºÎ­Ï‚ (MRR, NDCG) Ï€Î±ÏÎ¿Ï…ÏƒÎ¹Î¬Î¶Î¿Î½Ï„Î±Î¹ Î³Î¹Î± ÏƒÏ‡ÎµÏ„Î¹ÎºÎ® ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ· Î¼ÎµÏ„Î±Î¾Ï Ï„Ï‰Î½ ÏƒÏ…ÏƒÏ„Î·Î¼Î¬Ï„Ï‰Î½\n",
    "- Î¤Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ½Ï„Î±Î¹ Î³Î¹Î± Ï„Î·Î½ ÎµÏ€Î¹Î»Î¿Î³Î® Ï„Ï‰Î½ ÎºÎ±Î»ÏÏ„ÎµÏÏ‰Î½ retrieval strategies Î³Î¹Î± Ï„Î¿ **Experiment 2** (reranking optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3d93f",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a401c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ matplotlib Î³Î¹Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬ (matching llm_judge_plots)\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['DejaVu Serif', 'Times New Roman', 'Liberation Serif']\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Larger font sizes for report readability\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 15\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['figure.titlesize'] = 16\n",
    "\n",
    "# IBM Carbon color palette for the 5 retrieval methods\n",
    "ibm_colors = ['#648FFF', '#785EF0', '#DC267F', '#FE6100', '#FFB000']\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=ibm_colors)\n",
    "\n",
    "print(\"âœ“ Î’Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚ Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51258afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "RESULTS_DIR = Path('../../results/experiment_1')\n",
    "OUTPUT_DIR = Path('../../output/experiment_1_plots')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data files (corrected file patterns)\n",
    "summary_file = list(RESULTS_DIR.glob('experiment_summary_full_*.csv'))[-1]\n",
    "stats_file = list(RESULTS_DIR.glob('experiment_statistical_analysis_full_*.csv'))[-1]\n",
    "full_results_file = list(RESULTS_DIR.glob('experiment_full_results_full_*.json'))[-1]\n",
    "\n",
    "print(f\"Loading data from:\")\n",
    "print(f\"  Summary: {summary_file.name}\")\n",
    "print(f\"  Statistics: {stats_file.name}\")\n",
    "print(f\"  Full Results: {full_results_file.name}\")\n",
    "\n",
    "# Load CSV files\n",
    "df_summary = pd.read_csv(summary_file)\n",
    "df_stats = pd.read_csv(stats_file)\n",
    "\n",
    "# Load JSON file\n",
    "with open(full_results_file, 'r') as f:\n",
    "    full_results = json.load(f)\n",
    "\n",
    "print(\"\\nâœ… Data loaded successfully\")\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Scenarios: {len(df_summary)}\")\n",
    "print(f\"  Statistical comparisons: {len(df_stats)}\")\n",
    "print(f\"  Total queries per scenario: {df_summary['total_queries'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb1f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics (including k=10 for Experiment 2 optimization)\n",
    "print(\"\\nğŸ“Š Summary Table\\n\")\n",
    "display_cols = ['scenario', 'retrieval_type', 'precision@10_mean', 'recall@10_mean', \n",
    "                'f1@10_mean', 'map_mean', 'mrr_mean', 'ndcg@10_mean', 'time_mean_ms']\n",
    "df_summary[display_cols].round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e50f7c",
   "metadata": {},
   "source": [
    "## 2. Data Preparation & Color Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59bf7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM Carbon Categorical Color Palette (Light theme)\n",
    "# Curated sequence for maximum contrast between neighboring colors\n",
    "CARBON_COLORS = {\n",
    "    'purple_70': '#6929c4',   # 01. Purple 70\n",
    "    'cyan_50': '#1192e8',     # 02. Cyan 50\n",
    "    'teal_70': '#005d5d',     # 03. Teal 70\n",
    "    'magenta_70': '#9f1853',  # 04. Magenta 70\n",
    "    'red_50': '#fa4d56',      # 05. Red 50\n",
    "    'red_90': '#570408',      # 06. Red 90\n",
    "    'green_60': '#198038',    # 07. Green 60\n",
    "    'blue_80': '#002d9c',     # 08. Blue 80\n",
    "    'magenta_50': '#ee538b',  # 09. Magenta 50\n",
    "    'yellow_50': '#b28600',   # 10. Yellow 50\n",
    "    'teal_50': '#009d9a',     # 11. Teal 50\n",
    "    'cyan_90': '#012749',     # 12. Cyan 90\n",
    "    'orange_70': '#8a3800',   # 13. Orange 70\n",
    "    'purple_50': '#a56eff'    # 14. Purple 50\n",
    "}\n",
    "\n",
    "# Apply Carbon colors to retrieval methods (5 distinct methods)\n",
    "# Using sequence order for maximum contrast between neighbors\n",
    "COLOR_SCHEME = {\n",
    "    'BM25_Baseline': CARBON_COLORS['purple_70'],        # 01. Purple - Traditional baseline\n",
    "    'SPLADE_Baseline': CARBON_COLORS['cyan_50'],        # 02. Cyan - Neural sparse  \n",
    "    'Dense_BGE_M3': CARBON_COLORS['green_60'],          # 07. Green - Dense semantic (skipped for contrast)\n",
    "    'Hybrid_SPLADE_BGE_M3': CARBON_COLORS['magenta_70'], # 04. Magenta - Best hybrid\n",
    "    'Hybrid_BM25_BGE_M3': CARBON_COLORS['yellow_50']    # 10. Yellow - Traditional hybrid\n",
    "}\n",
    "\n",
    "# For reference - named colors\n",
    "colors_thesis = {\n",
    "    'primary': CARBON_COLORS['cyan_50'],\n",
    "    'secondary': CARBON_COLORS['purple_70'],\n",
    "    'accent': CARBON_COLORS['magenta_70'],\n",
    "    'success': CARBON_COLORS['green_60'],\n",
    "    'warning': CARBON_COLORS['yellow_50'],\n",
    "    'danger': CARBON_COLORS['red_50'],\n",
    "    'grid': '#e0e0e0',       # Light gray for grid\n",
    "    'text': '#161616'        # Carbon text color\n",
    "}\n",
    "\n",
    "# Readable labels for plots\n",
    "LABEL_MAPPING = {\n",
    "    'BM25_Baseline': 'BM25',\n",
    "    'SPLADE_Baseline': 'SPLADE',\n",
    "    'Dense_BGE_M3': 'Dense\\n(BGE-M3)',\n",
    "    'Hybrid_SPLADE_BGE_M3': 'Hybrid\\n(SPLADE+BGE)',\n",
    "    'Hybrid_BM25_BGE_M3': 'Hybrid\\n(BM25+BGE)'\n",
    "}\n",
    "\n",
    "# Short labels for compact plots\n",
    "SHORT_LABELS = {\n",
    "    'BM25_Baseline': 'BM25',\n",
    "    'SPLADE_Baseline': 'SPLADE',\n",
    "    'Dense_BGE_M3': 'Dense',\n",
    "    'Hybrid_SPLADE_BGE_M3': 'Hybrid-S',\n",
    "    'Hybrid_BM25_BGE_M3': 'Hybrid-B'\n",
    "}\n",
    "\n",
    "# Order for plots\n",
    "SCENARIO_ORDER = ['BM25_Baseline', 'SPLADE_Baseline', 'Dense_BGE_M3', \n",
    "                  'Hybrid_SPLADE_BGE_M3', 'Hybrid_BM25_BGE_M3']\n",
    "\n",
    "print(\"âœ… IBM Carbon color palette applied (categorical - maximized contrast)\")\n",
    "print(\"   Colors: Purple â†’ Cyan â†’ Green â†’ Magenta â†’ Yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d064da4",
   "metadata": {},
   "source": [
    "## 3. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(fig, filename, formats=['png', 'pdf']):\n",
    "    \"\"\"Save figure in multiple formats for thesis.\"\"\"\n",
    "    for fmt in formats:\n",
    "        filepath = OUTPUT_DIR / f\"{filename}.{fmt}\"\n",
    "        fig.savefig(filepath, format=fmt, bbox_inches='tight', dpi=300)\n",
    "        print(f\"  âœ“ Saved: {filepath}\")\n",
    "\n",
    "def calculate_ci_95(mean, std, n=506):\n",
    "    \"\"\"Calculate 95% confidence interval (Â±1.96 * SE).\"\"\"\n",
    "    se = std / np.sqrt(n)\n",
    "    ci = 1.96 * se\n",
    "    return ci\n",
    "\n",
    "def format_percentage(value, decimals=1):\n",
    "    \"\"\"Format value as percentage.\"\"\"\n",
    "    return f\"{value*100:.{decimals}f}%\"\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1f02c",
   "metadata": {},
   "source": [
    "## 4. Metric Interpretation Strategy\n",
    "\n",
    "### Understanding Dataset Limitations\n",
    "\n",
    "The **SOSum dataset** uses Stack Overflow's default ranking system (community voting + accepted answers), which doesn't always represent perfect semantic ordering. This affects **rank-aware metrics** (NDCG, MRR) that assume the first answer is most relevant.\n",
    "\n",
    "**Our Approach:**\n",
    "1. **Primary Focus**: Set-based metrics (Precision@k, Recall@k, F1@k, MAP) - less sensitive to ranking noise\n",
    "2. **Secondary Analysis**: Rank-aware metrics (NDCG, MRR) - valuable for *relative comparison* between methods\n",
    "3. **Confidence Intervals**: Use 95% CI instead of raw standard deviation for cleaner visualization\n",
    "\n",
    "**Rationale**: All retrieval methods face the same noisy ground truth, so relative comparisons remain valid. Real-world data with inherent imperfections is preferable to artificial benchmarks.\n",
    "\n",
    "**Note**: Error bars in all plots show 95% confidence intervals (Â±1.96 Ã— SE), not standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ed6de",
   "metadata": {},
   "source": [
    "## 5. Figure 1: Overall Performance Comparison (MAP, MRR, NDCG@10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b5bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for overall performance metrics\n",
    "metrics_to_plot = ['map_mean', 'mrr_mean', 'ndcg@10_mean']\n",
    "metric_labels = ['MAP', 'MRR', 'NDCG@10']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # Larger size\n",
    "fig.suptitle('Î£Ï…Î½Î¿Î»Î¹ÎºÎ® ÎµÏ€Î¯Î´Î¿ÏƒÎ· Ï„Î¿Ï… ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚ Î±Î½Î¬ÎºÏ„Î·ÏƒÎ·Ï‚', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for idx, (metric, label) in enumerate(zip(metrics_to_plot, metric_labels)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Extract data\n",
    "    data = df_summary.set_index('scenario').loc[SCENARIO_ORDER]\n",
    "    values = data[metric].values\n",
    "    std_col = metric.replace('_mean', '_std')\n",
    "    stds = data[std_col].values\n",
    "    # Use 95% CI for cleaner visualization\n",
    "    errors = [calculate_ci_95(v, s) for v, s in zip(values, stds)]\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.bar(range(len(SCENARIO_ORDER)), values, \n",
    "                   color=[COLOR_SCHEME[s] for s in SCENARIO_ORDER],\n",
    "                   yerr=errors, capsize=5, alpha=0.85, edgecolor='black', \n",
    "                   linewidth=1.0, error_kw={'linewidth': 1.5, 'alpha': 0.7})\n",
    "    \n",
    "    # Customize\n",
    "    ax.set_ylabel(label, fontweight='bold', fontsize=14)\n",
    "    ax.set_xticks(range(len(SCENARIO_ORDER)))\n",
    "    ax.set_xticklabels([SHORT_LABELS[s] for s in SCENARIO_ORDER], rotation=45, ha='right', fontsize=12)\n",
    "    ax.set_ylim(0, max(values) * 1.2)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, values)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + errors[i] + 0.01,\n",
    "                f'{val:.3f}',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Highlight best performer\n",
    "    best_idx = np.argmax(values)\n",
    "    bars[best_idx].set_edgecolor('gold')\n",
    "    bars[best_idx].set_linewidth(3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig1_overall_performance')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Figure 1 generated: Overall Performance Comparison\")\n",
    "print(\"   Note: Error bars show 95% confidence intervals (not std)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978ce75",
   "metadata": {},
   "source": [
    "## 6. Figure 2: Precision@k Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4618ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision at different k values (including k=10 for Experiment 2)\n",
    "k_values = [1, 3, 5, 10]\n",
    "precision_metrics = [f'precision@{k}_mean' for k in k_values]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))  # Larger size\n",
    "\n",
    "x = np.arange(len(k_values))\n",
    "width = 0.15\n",
    "\n",
    "for i, scenario in enumerate(SCENARIO_ORDER):\n",
    "    data = df_summary[df_summary['scenario'] == scenario]\n",
    "    values = [data[metric].values[0] for metric in precision_metrics]\n",
    "    # Use 95% CI instead of std for cleaner visualization\n",
    "    stds = [data[metric.replace('_mean', '_std')].values[0] for metric in precision_metrics]\n",
    "    errors = [calculate_ci_95(v, s) for v, s in zip(values, stds)]\n",
    "    \n",
    "    offset = (i - len(SCENARIO_ORDER)/2 + 0.5) * width\n",
    "    bars = ax.bar(x + offset, values, width, label=SHORT_LABELS[scenario],\n",
    "                   color=COLOR_SCHEME[scenario], alpha=0.85, \n",
    "                   yerr=errors, capsize=3, edgecolor='black', linewidth=0.8, \n",
    "                   error_kw={'linewidth': 1.5, 'alpha': 0.7})\n",
    "\n",
    "ax.set_xlabel('k (Ï€Î»Î®Î¸Î¿Ï‚ Î±Î½Î±ÎºÏ„Î·Î¼Î­Î½Ï‰Î½ Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½)', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('Precision@k', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Î‘ÎºÏÎ¯Î²ÎµÎ¹Î± Î³Î¹Î± Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Ï„Î¹Î¼Î­Ï‚ Î±Ï€Î¿ÎºÎ¿Ï€Î®Ï‚', \n",
    "             fontsize=15, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'k={k}' for k in k_values], fontsize=12)\n",
    "ax.legend(loc='upper right', framealpha=0.95, edgecolor='gray', fontsize=12)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig2_precision_at_k')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Figure 2 generated: Precision@k Comparison (with k=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed552b7",
   "metadata": {},
   "source": [
    "## 6. Figure 3: Recall@k Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall at different k values (including k=10)\n",
    "recall_metrics = [f'recall@{k}_mean' for k in k_values]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))  # Larger size\n",
    "\n",
    "x = np.arange(len(k_values))\n",
    "width = 0.15\n",
    "\n",
    "for i, scenario in enumerate(SCENARIO_ORDER):\n",
    "    data = df_summary[df_summary['scenario'] == scenario]\n",
    "    values = [data[metric].values[0] for metric in recall_metrics]\n",
    "    # Use 95% CI instead of std\n",
    "    stds = [data[metric.replace('_mean', '_std')].values[0] for metric in recall_metrics]\n",
    "    errors = [calculate_ci_95(v, s) for v, s in zip(values, stds)]\n",
    "    \n",
    "    offset = (i - len(SCENARIO_ORDER)/2 + 0.5) * width\n",
    "    bars = ax.bar(x + offset, values, width, label=SHORT_LABELS[scenario],\n",
    "                   color=COLOR_SCHEME[scenario], alpha=0.85,\n",
    "                   yerr=errors, capsize=3, edgecolor='black', linewidth=0.8,\n",
    "                   error_kw={'linewidth': 1.5, 'alpha': 0.7})\n",
    "\n",
    "ax.set_xlabel('k (Ï€Î»Î®Î¸Î¿Ï‚ Î±Î½Î±ÎºÏ„Î·Î¼Î­Î½Ï‰Î½ Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½)', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('Recall@k', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Î‘Î½Î¬ÎºÎ»Î·ÏƒÎ· Î³Î¹Î± Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Ï„Î¹Î¼Î­Ï‚ Î±Ï€Î¿ÎºÎ¿Ï€Î®Ï‚', \n",
    "             fontsize=15, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'k={k}' for k in k_values], fontsize=12)\n",
    "ax.legend(loc='upper left', framealpha=0.95, edgecolor='gray', fontsize=12)\n",
    "ax.set_ylim(0, max([df_summary[metric].max() for metric in recall_metrics]) * 1.15)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig3_recall_at_k')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Figure 3 generated: Recall@k Comparison (with k=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c5466f",
   "metadata": {},
   "source": [
    "## 7. Figure 4: F1@k Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 scores at different k values (including k=10 for consistency)\n",
    "f1_k_values = [3, 5, 10]\n",
    "f1_metrics = [f'f1@{k}_mean' for k in f1_k_values]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))  # Larger size\n",
    "\n",
    "x = np.arange(len(f1_k_values))\n",
    "width = 0.15\n",
    "\n",
    "for i, scenario in enumerate(SCENARIO_ORDER):\n",
    "    data = df_summary[df_summary['scenario'] == scenario]\n",
    "    values = [data[metric].values[0] for metric in f1_metrics]\n",
    "    stds = [data[metric.replace('_mean', '_std')].values[0] for metric in f1_metrics]\n",
    "    errors = [calculate_ci_95(v, s) for v, s in zip(values, stds)]\n",
    "    \n",
    "    offset = (i - len(SCENARIO_ORDER)/2 + 0.5) * width\n",
    "    bars = ax.bar(x + offset, values, width, label=SHORT_LABELS[scenario],\n",
    "                   color=COLOR_SCHEME[scenario], alpha=0.85,\n",
    "                   yerr=errors, capsize=3, edgecolor='black', linewidth=0.8,\n",
    "                   error_kw={'linewidth': 1.5, 'alpha': 0.7})\n",
    "\n",
    "ax.set_xlabel('k (Ï€Î»Î®Î¸Î¿Ï‚ Î±Î½Î±ÎºÏ„Î·Î¼Î­Î½Ï‰Î½ Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½)', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('F1@k', fontweight='bold', fontsize=14)\n",
    "ax.set_title('F1 Î³Î¹Î± Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Ï„Î¹Î¼Î­Ï‚ Î±Ï€Î¿ÎºÎ¿Ï€Î®Ï‚', \n",
    "             fontsize=15, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'k={k}' for k in f1_k_values], fontsize=12)\n",
    "ax.legend(loc='lower right', framealpha=0.95, edgecolor='gray', fontsize=12)\n",
    "ax.set_ylim(0, max([df_summary[metric].max() for metric in f1_metrics]) * 1.15)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig4_f1_scores')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Figure 4 generated: F1@k Scores (with k=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2687d564",
   "metadata": {},
   "source": [
    "## 8. Figure 5: Precision-Recall Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20008aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall scatter plot at k=5\n",
    "fig, ax = plt.subplots(figsize=(12, 10))  # Larger size\n",
    "\n",
    "for scenario in SCENARIO_ORDER:\n",
    "    data = df_summary[df_summary['scenario'] == scenario]\n",
    "    precision = data['precision@5_mean'].values[0]\n",
    "    recall = data['recall@5_mean'].values[0]\n",
    "    p_std = data['precision@5_std'].values[0]\n",
    "    r_std = data['recall@5_std'].values[0]\n",
    "    \n",
    "    # Use 95% CI for error bars\n",
    "    p_ci = calculate_ci_95(precision, p_std)\n",
    "    r_ci = calculate_ci_95(recall, r_std)\n",
    "    \n",
    "    # Plot point with error bars\n",
    "    ax.errorbar(recall, precision, xerr=r_ci, yerr=p_ci,\n",
    "                marker='o', markersize=16, label=SHORT_LABELS[scenario],\n",
    "                color=COLOR_SCHEME[scenario], capsize=6, capthick=2.5,\n",
    "                linewidth=3, alpha=0.8)\n",
    "\n",
    "# Add quadrant lines\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.4, linewidth=1.5)\n",
    "ax.axvline(x=0.3, color='gray', linestyle='--', alpha=0.4, linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Recall@5', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('Precision@5', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Î£Ï…Î¼Î²Î¹Î²Î±ÏƒÎ¼ÏŒÏ‚ Î±ÎºÏÎ¯Î²ÎµÎ¹Î±Ï‚-Î±Î½Î¬ÎºÎ»Î·ÏƒÎ·Ï‚ Î³Î¹Î± k = 5\\n', \n",
    "             fontsize=15, fontweight='bold', pad=15)\n",
    "ax.legend(loc='lower left', framealpha=0.95, edgecolor='gray', fontsize=12)\n",
    "ax.grid(True, alpha=0.3, linestyle=':')\n",
    "ax.set_xlim(0, max(df_summary['recall@5_mean']) * 1.15)\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "# Add text annotation for ideal quadrant\n",
    "ax.text(0.45, 0.95, 'Î¥ÏˆÎ·Î»Î® Î±ÎºÏÎ¯Î²ÎµÎ¹Î±\\nÎ¥ÏˆÎ·Î»Î® Î±Î½Î¬ÎºÎ»Î·ÏƒÎ·', ha='center', fontsize=11, \n",
    "        style='italic', alpha=0.6, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.3, edgecolor='gray'))\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig5_precision_recall_tradeoff')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Figure 5 generated: Precision-Recall Tradeoff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97290a9d",
   "metadata": {},
   "source": [
    "## 9. Figure 6: NDCG@k Progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a783c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG at different cutoffs (including k=10)\n",
    "ndcg_k_values = [1, 3, 5, 10]\n",
    "ndcg_metrics = [f'ndcg@{k}_mean' for k in ndcg_k_values]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))  # Larger size\n",
    "\n",
    "for scenario in SCENARIO_ORDER:\n",
    "    data = df_summary[df_summary['scenario'] == scenario]\n",
    "    values = [data[metric].values[0] for metric in ndcg_metrics]\n",
    "    \n",
    "    ax.plot(ndcg_k_values, values, marker='o', markersize=12, \n",
    "            linewidth=3.5, label=SHORT_LABELS[scenario],\n",
    "            color=COLOR_SCHEME[scenario], alpha=0.85)\n",
    "\n",
    "ax.set_xlabel('k', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('NDCG@k', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Normalized Discounted Cumulative Gain Î³Î¹Î± Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Ï„Î¹Î¼Î­Ï‚ Î±Ï€Î¿ÎºÎ¿Ï€Î®Ï‚', \n",
    "             fontsize=15, fontweight='bold', pad=15)\n",
    "ax.set_xticks(ndcg_k_values)\n",
    "ax.set_xticklabels([f'k={k}' for k in ndcg_k_values], fontsize=12)\n",
    "ax.legend(loc='lower left', framealpha=0.95, edgecolor='gray', fontsize=12)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(True, alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig6_ndcg_progression')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Figure 6 generated: NDCG@k Progression (with k=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a445f51",
   "metadata": {},
   "source": [
    "## 10. Figure 7: Query Latency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency comparison (without error bars due to high local variance)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))  # Larger size\n",
    "\n",
    "# Plot 1: Mean latency (no error bars to avoid obscuring)\n",
    "ax = axes[0]\n",
    "data = df_summary.set_index('scenario').loc[SCENARIO_ORDER]\n",
    "means = data['time_mean_ms'].values\n",
    "\n",
    "bars = ax.barh(range(len(SCENARIO_ORDER)), means,\n",
    "               color=[COLOR_SCHEME[s] for s in SCENARIO_ORDER],\n",
    "               alpha=0.85, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "ax.set_yticks(range(len(SCENARIO_ORDER)))\n",
    "ax.set_yticklabels([SHORT_LABELS[s] for s in SCENARIO_ORDER], fontsize=12)\n",
    "ax.set_xlabel('ÎœÎ­ÏƒÎ· ÎšÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ· (ms)', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· ÎšÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ·Ï‚ Î•ÏÏ‰Ï„Î·Î¼Î¬Ï„Ï‰Î½\\n(Î¤Î¿Ï€Î¹ÎºÎ® ÎµÎºÏ„Î­Î»ÎµÏƒÎ· - Ï…ÏˆÎ·Î»Î® Î´Î¹Î±ÏƒÏ€Î¿ÏÎ¬)', \n",
    "             fontweight='bold', fontsize=15)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle=':')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, mean) in enumerate(zip(bars, means)):\n",
    "    ax.text(mean + 20, bar.get_y() + bar.get_height()/2,\n",
    "            f'{mean:.1f} ms',\n",
    "            va='center', ha='left', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 2: Latency percentiles (cleaner visualization)\n",
    "ax = axes[1]\n",
    "metrics_latency = ['time_median_ms', 'time_p95_ms']\n",
    "x = np.arange(len(SCENARIO_ORDER))\n",
    "width = 0.35\n",
    "\n",
    "median_vals = data['time_median_ms'].values\n",
    "p95_vals = data['time_p95_ms'].values\n",
    "\n",
    "bars1 = ax.bar(x - width/2, median_vals, width, label='Î”Î¹Î¬Î¼ÎµÏƒÎ¿Ï‚ (P50)',\n",
    "               color=COLORS[0], alpha=0.85, \n",
    "               edgecolor='black', linewidth=1)\n",
    "bars2 = ax.bar(x + width/2, p95_vals, width, label='P95',\n",
    "               color=COLORS[3], alpha=0.85, \n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "ax.set_ylabel('ÎšÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ· (ms)', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Î•ÎºÎ±Ï„Î¿ÏƒÏ„Î·Î¼ÏŒÏÎ¹Î± ÎšÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ·Ï‚', fontweight='bold', fontsize=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([SHORT_LABELS[s] for s in SCENARIO_ORDER], \n",
    "                   rotation=45, ha='right', fontsize=11)\n",
    "ax.legend(framealpha=0.95, edgecolor='gray', fontsize=12)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'fig7_latency_analysis')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Figure 7 generated: Query Latency Analysis (cleaned up)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448d43c",
   "metadata": {},
   "source": [
    "## 11. Figure 8: Statistical Significance Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4988a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairwise comparison matrix for MAP metric\n",
    "map_stats = df_stats[df_stats['metric'] == 'map'].copy()\n",
    "\n",
    "# Create matrix\n",
    "scenarios = SCENARIO_ORDER\n",
    "n = len(scenarios)\n",
    "significance_matrix = np.zeros((n, n))\n",
    "effect_size_matrix = np.zeros((n, n))\n",
    "\n",
    "for _, row in map_stats.iterrows():\n",
    "    if row['method1'] in scenarios and row['method2'] in scenarios:\n",
    "        i = scenarios.index(row['method1'])\n",
    "        j = scenarios.index(row['method2'])\n",
    "        \n",
    "        # Significance (1 = significant, 0 = not significant)\n",
    "        significance_matrix[i, j] = 1 if row['bonferroni_significant'] else 0\n",
    "        significance_matrix[j, i] = 1 if row['bonferroni_significant'] else 0  # Symmetric\n",
    "        \n",
    "        # Effect size (symmetric but with opposite sign)\n",
    "        if pd.notna(row['effect_size']):\n",
    "            effect_size_matrix[i, j] = row['effect_size']\n",
    "            effect_size_matrix[j, i] = -row['effect_size']  # Opposite sign for symmetric position\n",
    "\n",
    "# Create improved heatmap with better styling\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 9))  # Larger size\n",
    "fig.suptitle('Statistical Significance Analysis - MAP Metric', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "# --- LEFT: Significance heatmap with cleaner design ---\n",
    "ax = axes[0]\n",
    "\n",
    "# Use discrete colormap for binary significance\n",
    "from matplotlib.colors import ListedColormap\n",
    "sig_colors = ['#fee5d9', '#a50f15']  # Light red (non-sig) to dark red (sig)\n",
    "sig_cmap = ListedColormap(sig_colors)\n",
    "\n",
    "im1 = ax.imshow(significance_matrix, cmap=sig_cmap, aspect='auto', vmin=0, vmax=1, \n",
    "                interpolation='nearest', alpha=0.9)\n",
    "\n",
    "# Add gridlines for clarity\n",
    "for i in range(n+1):\n",
    "    ax.axhline(i-0.5, color='white', linewidth=2.5)\n",
    "    ax.axvline(i-0.5, color='white', linewidth=2.5)\n",
    "\n",
    "ax.set_xticks(np.arange(n))\n",
    "ax.set_yticks(np.arange(n))\n",
    "ax.set_xticklabels([SHORT_LABELS[s] for s in scenarios], rotation=45, ha='right', fontsize=13)\n",
    "ax.set_yticklabels([SHORT_LABELS[s] for s in scenarios], fontsize=13)\n",
    "ax.set_title('Statistical Significance\\n(Bonferroni Corrected p < 0.05)', \n",
    "             fontweight='bold', pad=20, fontsize=15)\n",
    "\n",
    "# Add clean annotations with stars for significance\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i == j:\n",
    "            # Diagonal - same method\n",
    "            ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, \n",
    "                                       fill=True, facecolor='lightgray', \n",
    "                                       edgecolor='white', linewidth=2.5, alpha=0.5))\n",
    "            ax.text(j, i, 'â€”', ha=\"center\", va=\"center\", \n",
    "                   color='gray', fontsize=20, fontweight='bold')\n",
    "        else:\n",
    "            # Off-diagonal - show significance\n",
    "            if significance_matrix[i, j] == 1:\n",
    "                ax.text(j, i, '***', ha=\"center\", va=\"center\", \n",
    "                       color='white', fontsize=24, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(j, i, 'n.s.', ha=\"center\", va=\"center\", \n",
    "                       color='#666', fontsize=11, style='italic', alpha=0.7)\n",
    "\n",
    "# Custom colorbar\n",
    "cbar1 = plt.colorbar(im1, ax=ax, ticks=[0.25, 0.75], pad=0.02)\n",
    "cbar1.ax.set_yticklabels(['Not Significant', 'Significant'], fontsize=11)\n",
    "cbar1.ax.tick_params(size=0)\n",
    "\n",
    "# --- RIGHT: Effect size heatmap with improved styling ---\n",
    "ax = axes[1]\n",
    "\n",
    "# Use diverging colormap centered at 0\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "norm = TwoSlopeNorm(vmin=-1.5, vcenter=0, vmax=1.5)\n",
    "im2 = ax.imshow(effect_size_matrix, cmap='RdBu_r', aspect='auto', norm=norm, \n",
    "                interpolation='nearest', alpha=0.9)\n",
    "\n",
    "# Add gridlines\n",
    "for i in range(n+1):\n",
    "    ax.axhline(i-0.5, color='white', linewidth=2.5)\n",
    "    ax.axvline(i-0.5, color='white', linewidth=2.5)\n",
    "\n",
    "ax.set_xticks(np.arange(n))\n",
    "ax.set_yticks(np.arange(n))\n",
    "ax.set_xticklabels([SHORT_LABELS[s] for s in scenarios], rotation=45, ha='right', fontsize=13)\n",
    "ax.set_yticklabels([SHORT_LABELS[s] for s in scenarios], fontsize=13)\n",
    "ax.set_title('Effect Size (Cohen\\'s d)\\nPositive = Row > Column', \n",
    "             fontweight='bold', pad=20, fontsize=15)\n",
    "\n",
    "# Add clean annotations with better visibility\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i == j:\n",
    "            # Diagonal\n",
    "            ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, \n",
    "                                       fill=True, facecolor='lightgray', \n",
    "                                       edgecolor='white', linewidth=2.5, alpha=0.5))\n",
    "            ax.text(j, i, 'â€”', ha=\"center\", va=\"center\", \n",
    "                   color='gray', fontsize=20, fontweight='bold')\n",
    "        elif effect_size_matrix[i, j] != 0:\n",
    "            # Show effect size with color-adaptive text\n",
    "            value = effect_size_matrix[i, j]\n",
    "            # Choose text color based on background intensity\n",
    "            text_color = 'white' if abs(value) > 0.5 else 'black'\n",
    "            \n",
    "            ax.text(j, i, f'{value:.2f}', ha=\"center\", va=\"center\", \n",
    "                   color=text_color, fontsize=13, fontweight='bold')\n",
    "            \n",
    "            # Add effect size interpretation as subscript\n",
    "            if abs(value) >= 0.8:\n",
    "                size_label = 'L'  # Large\n",
    "            elif abs(value) >= 0.5:\n",
    "                size_label = 'M'  # Medium\n",
    "            elif abs(value) >= 0.2:\n",
    "                size_label = 'S'  # Small\n",
    "            else:\n",
    "                size_label = ''\n",
    "            \n",
    "            if size_label:\n",
    "                ax.text(j, i+0.35, size_label, ha=\"center\", va=\"center\", \n",
    "                       color=text_color, fontsize=9, style='italic', alpha=0.8)\n",
    "\n",
    "# Custom colorbar with interpretations\n",
    "cbar2 = plt.colorbar(im2, ax=ax, pad=0.02)\n",
    "cbar2.set_label('Effect Size', fontsize=12, fontweight='bold')\n",
    "cbar2.ax.tick_params(labelsize=11)\n",
    "\n",
    "# Add effect size interpretation legend\n",
    "legend_text = 'S: Small (0.2)\\nM: Medium (0.5)\\nL: Large (0.8)'\n",
    "ax.text(1.25, -0.15, legend_text, transform=ax.transAxes,\n",
    "        fontsize=10, verticalalignment='top', style='italic',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.3, edgecolor='gray'))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "save_figure(fig, 'fig8_statistical_significance')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Figure 8 generated: Statistical Significance Analysis (improved design)\")\n",
    "print(\"   - Stars (***) indicate statistical significance (p < 0.05)\")\n",
    "print(\"   - Effect sizes labeled with magnitude (S/M/L)\")\n",
    "print(\"   - n.s. = not significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341f2c7",
   "metadata": {},
   "source": [
    "## 12. Figure 9: Comprehensive Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f40cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary figure\n",
    "fig = plt.figure(figsize=(22, 12))  # Much larger size\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.35)\n",
    "\n",
    "# 1. MAP comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "data = df_summary.set_index('scenario').loc[SCENARIO_ORDER]\n",
    "bars = ax1.bar(range(len(SCENARIO_ORDER)), data['map_mean'].values,\n",
    "               color=[COLOR_SCHEME[s] for s in SCENARIO_ORDER], alpha=0.85,\n",
    "               edgecolor='black', linewidth=1.2)\n",
    "ax1.set_title('MAP', fontweight='bold', fontsize=14)\n",
    "ax1.set_xticks(range(len(SCENARIO_ORDER)))\n",
    "ax1.set_xticklabels([SHORT_LABELS[s] for s in SCENARIO_ORDER], rotation=45, ha='right', fontsize=11)\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "ax1.tick_params(axis='y', labelsize=11)\n",
    "# Highlight best\n",
    "best_idx = np.argmax(data['map_mean'].values)\n",
    "bars[best_idx].set_edgecolor('gold')\n",
    "bars[best_idx].set_linewidth(3.5)\n",
    "\n",
    "# 2. MRR comparison\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "bars = ax2.bar(range(len(SCENARIO_ORDER)), data['mrr_mean'].values,\n",
    "               color=[COLOR_SCHEME[s] for s in SCENARIO_ORDER], alpha=0.85,\n",
    "               edgecolor='black', linewidth=1.2)\n",
    "ax2.set_title('MRR*', fontweight='bold', fontsize=14)\n",
    "ax2.set_xticks(range(len(SCENARIO_ORDER)))\n",
    "ax2.set_xticklabels([SHORT_LABELS[s] for s in SCENARIO_ORDER], rotation=45, ha='right', fontsize=11)\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "ax2.tick_params(axis='y', labelsize=11)\n",
    "best_idx = np.argmax(data['mrr_mean'].values)\n",
    "bars[best_idx].set_edgecolor('gold')\n",
    "bars[best_idx].set_linewidth(3.5)\n",
    "\n",
    "# 3. NDCG@10 comparison\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "bars = ax3.bar(range(len(SCENARIO_ORDER)), data['ndcg@10_mean'].values,\n",
    "               color=[COLOR_SCHEME[s] for s in SCENARIO_ORDER], alpha=0.85,\n",
    "               edgecolor='black', linewidth=1.2)\n",
    "ax3.set_title('NDCG@10*', fontweight='bold', fontsize=14)\n",
    "ax3.set_xticks(range(len(SCENARIO_ORDER)))\n",
    "ax3.set_xticklabels([SHORT_LABELS[s] for s in SCENARIO_ORDER], rotation=45, ha='right', fontsize=11)\n",
    "ax3.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "ax3.tick_params(axis='y', labelsize=11)\n",
    "best_idx = np.argmax(data['ndcg@10_mean'].values)\n",
    "bars[best_idx].set_edgecolor('gold')\n",
    "bars[best_idx].set_linewidth(3.5)\n",
    "\n",
    "# 4. Precision@k curves (with k=10)\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "k_vals = [1, 3, 5, 10]\n",
    "for scenario in SCENARIO_ORDER:\n",
    "    scenario_data = df_summary[df_summary['scenario'] == scenario]\n",
    "    values = [scenario_data[f'precision@{k}_mean'].values[0] for k in k_vals]\n",
    "    ax4.plot(k_vals, values, marker='o', markersize=13, \n",
    "            linewidth=3.5, label=SHORT_LABELS[scenario],\n",
    "            color=COLOR_SCHEME[scenario], alpha=0.85)\n",
    "ax4.set_xlabel('k', fontweight='bold', fontsize=14)\n",
    "ax4.set_ylabel('Precision@k', fontweight='bold', fontsize=14)\n",
    "ax4.set_title('Precision at Different Cutoffs', fontweight='bold', fontsize=15)\n",
    "ax4.legend(loc='upper right', ncol=5, framealpha=0.95, fontsize=12, edgecolor='gray')\n",
    "ax4.grid(True, alpha=0.3, linestyle=':')\n",
    "ax4.set_xticks(k_vals)\n",
    "ax4.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "# 5. Recall@k curves (with k=10)\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "for scenario in SCENARIO_ORDER:\n",
    "    scenario_data = df_summary[df_summary['scenario'] == scenario]\n",
    "    values = [scenario_data[f'recall@{k}_mean'].values[0] for k in k_vals]\n",
    "    ax5.plot(k_vals, values, marker='s', markersize=13, \n",
    "            linewidth=3.5, label=SHORT_LABELS[scenario],\n",
    "            color=COLOR_SCHEME[scenario], alpha=0.85)\n",
    "ax5.set_xlabel('k', fontweight='bold', fontsize=14)\n",
    "ax5.set_ylabel('Recall@k', fontweight='bold', fontsize=14)\n",
    "ax5.set_title('Recall at Different Cutoffs', fontweight='bold', fontsize=15)\n",
    "ax5.legend(loc='lower right', ncol=5, framealpha=0.95, fontsize=12, edgecolor='gray')\n",
    "ax5.grid(True, alpha=0.3, linestyle=':')\n",
    "ax5.set_xticks(k_vals)\n",
    "ax5.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "fig.suptitle('Experiment 1: Retrieval Strategy Comprehensive Comparison\\n(*Rank-aware metrics)', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "save_figure(fig, 'fig9_comprehensive_dashboard')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Figure 9 generated: Comprehensive Summary Dashboard (updated with k=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431019da",
   "metadata": {},
   "source": [
    "## 13. Summary Table for Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159277fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication-ready summary table\n",
    "summary_table = df_summary[['scenario', 'precision@5_mean', 'precision@5_std',\n",
    "                             'recall@5_mean', 'recall@5_std',\n",
    "                             'f1@5_mean', 'f1@5_std',\n",
    "                             'map_mean', 'map_std',\n",
    "                             'mrr_mean', 'mrr_std',\n",
    "                             'ndcg@5_mean', 'ndcg@5_std',\n",
    "                             'time_mean_ms', 'time_std_ms']].copy()\n",
    "\n",
    "# Rename columns for readability\n",
    "summary_table['scenario'] = summary_table['scenario'].map(SHORT_LABELS)\n",
    "summary_table.columns = ['Method', 'P@5', 'P@5_std', 'R@5', 'R@5_std', \n",
    "                         'F1@5', 'F1@5_std', 'MAP', 'MAP_std',\n",
    "                         'MRR', 'MRR_std', 'NDCG@5', 'NDCG@5_std',\n",
    "                         'Latency(ms)', 'Lat_std']\n",
    "\n",
    "# Format for display\n",
    "display_table = summary_table.copy()\n",
    "for col in display_table.columns[1:]:\n",
    "    if 'std' in col:\n",
    "        display_table[col] = display_table[col].apply(lambda x: f'Â±{x:.3f}')\n",
    "    elif 'Latency' in col:\n",
    "        display_table[col] = display_table[col].apply(lambda x: f'{x:.1f}')\n",
    "    else:\n",
    "        display_table[col] = display_table[col].apply(lambda x: f'{x:.3f}')\n",
    "\n",
    "# Combine mean and std\n",
    "result_table = pd.DataFrame({\n",
    "    'Method': display_table['Method'],\n",
    "    'Precision@5': display_table['P@5'] + ' ' + display_table['P@5_std'],\n",
    "    'Recall@5': display_table['R@5'] + ' ' + display_table['R@5_std'],\n",
    "    'F1@5': display_table['F1@5'] + ' ' + display_table['F1@5_std'],\n",
    "    'MAP': display_table['MAP'] + ' ' + display_table['MAP_std'],\n",
    "    'MRR': display_table['MRR'] + ' ' + display_table['MRR_std'],\n",
    "    'NDCG@5': display_table['NDCG@5'] + ' ' + display_table['NDCG@5_std'],\n",
    "    'Latency(ms)': display_table['Latency(ms)'] + ' ' + display_table['Lat_std']\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ“Š Summary Table for Thesis (Mean Â± Std)\\n\")\n",
    "print(result_table.to_string(index=False))\n",
    "\n",
    "# Save as CSV and LaTeX\n",
    "result_table.to_csv(OUTPUT_DIR / 'table1_summary_results.csv', index=False)\n",
    "result_table.to_latex(OUTPUT_DIR / 'table1_summary_results.tex', index=False, escape=False)\n",
    "\n",
    "print(\"\\nâœ… Summary table saved as CSV and LaTeX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0bdb54",
   "metadata": {},
   "source": [
    "## 14. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71872bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key findings\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS FROM EXPERIMENT 1\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Best performers\n",
    "best_map = df_summary.loc[df_summary['map_mean'].idxmax()]\n",
    "print(f\"ğŸ† BEST OVERALL (MAP): {SHORT_LABELS[best_map['scenario']]}\")\n",
    "print(f\"   MAP = {best_map['map_mean']:.4f} Â± {best_map['map_std']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Compare to baseline\n",
    "baseline = df_summary[df_summary['scenario'] == 'BM25_Baseline'].iloc[0]\n",
    "best_improvement = ((best_map['map_mean'] - baseline['map_mean']) / baseline['map_mean']) * 100\n",
    "print(f\"ğŸ“ˆ IMPROVEMENT OVER BM25 BASELINE: {best_improvement:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Precision leader\n",
    "best_precision = df_summary.loc[df_summary['precision@5_mean'].idxmax()]\n",
    "print(f\"ğŸ¯ HIGHEST PRECISION@5: {SHORT_LABELS[best_precision['scenario']]}\")\n",
    "print(f\"   P@5 = {best_precision['precision@5_mean']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Recall leader\n",
    "best_recall = df_summary.loc[df_summary['recall@5_mean'].idxmax()]\n",
    "print(f\"ğŸ” HIGHEST RECALL@5: {SHORT_LABELS[best_recall['scenario']]}\")\n",
    "print(f\"   R@5 = {best_recall['recall@5_mean']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Fastest\n",
    "fastest = df_summary.loc[df_summary['time_mean_ms'].idxmin()]\n",
    "print(f\"âš¡ FASTEST: {SHORT_LABELS[fastest['scenario']]}\")\n",
    "print(f\"   Latency = {fastest['time_mean_ms']:.1f} ms\")\n",
    "print()\n",
    "\n",
    "# Statistical significance summary\n",
    "sig_comparisons = df_stats[df_stats['bonferroni_significant'] == True]\n",
    "print(f\"ğŸ“Š STATISTICAL ANALYSIS:\")\n",
    "print(f\"   Total pairwise comparisons: {len(df_stats)}\")\n",
    "print(f\"   Statistically significant: {len(sig_comparisons)}\")\n",
    "print(f\"   Significance rate: {len(sig_comparisons)/len(df_stats)*100:.1f}%\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save findings to text file\n",
    "with open(OUTPUT_DIR / 'key_findings.txt', 'w') as f:\n",
    "    f.write(\"KEY FINDINGS FROM EXPERIMENT 1\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Best Overall (MAP): {SHORT_LABELS[best_map['scenario']]}\\n\")\n",
    "    f.write(f\"MAP = {best_map['map_mean']:.4f} Â± {best_map['map_std']:.4f}\\n\\n\")\n",
    "    f.write(f\"Improvement over BM25: {best_improvement:.1f}%\\n\\n\")\n",
    "    f.write(f\"Highest Precision@5: {SHORT_LABELS[best_precision['scenario']]} = {best_precision['precision@5_mean']:.4f}\\n\")\n",
    "    f.write(f\"Highest Recall@5: {SHORT_LABELS[best_recall['scenario']]} = {best_recall['recall@5_mean']:.4f}\\n\")\n",
    "    f.write(f\"Fastest: {SHORT_LABELS[fastest['scenario']]} = {fastest['time_mean_ms']:.1f} ms\\n\")\n",
    "\n",
    "print(\"\\nâœ… Key findings saved to key_findings.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fffbe8d",
   "metadata": {},
   "source": [
    "## 15. Export All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPORT SUMMARY\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(f\"ğŸ“ Output Directory: {OUTPUT_DIR}\\n\")\n",
    "print(\"Generated Files:\")\n",
    "print(\"  Figures (9 total):\")\n",
    "print(\"    âœ“ fig1_overall_performance.png/.pdf\")\n",
    "print(\"    âœ“ fig2_precision_at_k.png/.pdf\")\n",
    "print(\"    âœ“ fig3_recall_at_k.png/.pdf\")\n",
    "print(\"    âœ“ fig4_f1_scores.png/.pdf\")\n",
    "print(\"    âœ“ fig5_precision_recall_tradeoff.png/.pdf\")\n",
    "print(\"    âœ“ fig6_ndcg_progression.png/.pdf\")\n",
    "print(\"    âœ“ fig7_latency_analysis.png/.pdf\")\n",
    "print(\"    âœ“ fig8_statistical_significance.png/.pdf\")\n",
    "print(\"    âœ“ fig9_comprehensive_dashboard.png/.pdf\")\n",
    "print(\"\\n  Tables:\")\n",
    "print(\"    âœ“ table1_summary_results.csv\")\n",
    "print(\"    âœ“ table1_summary_results.tex\")\n",
    "print(\"\\n  Analysis:\")\n",
    "print(\"    âœ“ key_findings.txt\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nâœ… All visualizations and tables generated successfully!\")\n",
    "print(\"\\nğŸ“ Ready for thesis inclusion\")\n",
    "print(f\"\\nğŸ“ Use these figures in your thesis Chapter 4 (Experimental Evaluation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87999cb8",
   "metadata": {},
   "source": [
    "## Notes for Thesis\n",
    "\n",
    "### Recommended Figure Placement\n",
    "\n",
    "1. **Section 4.1.4.1 - Overall Performance**\n",
    "   - Figure 1: Overall Performance Comparison (MAP, MRR, NDCG@5)\n",
    "   - Table 1: Summary Results\n",
    "\n",
    "2. **Section 4.1.4.2 - Precision & Recall Analysis**\n",
    "   - Figure 2: Precision@k Comparison\n",
    "   - Figure 3: Recall@k Comparison\n",
    "   - Figure 5: Precision-Recall Tradeoff\n",
    "\n",
    "3. **Section 4.1.4.3 - Ranking Quality**\n",
    "   - Figure 6: NDCG@k Progression\n",
    "   - Figure 4: F1@k Scores\n",
    "\n",
    "4. **Section 4.1.4.4 - Computational Efficiency**\n",
    "   - Figure 7: Latency Analysis\n",
    "\n",
    "5. **Section 4.1.4.5 - Statistical Validation**\n",
    "   - Figure 8: Statistical Significance Heatmap\n",
    "\n",
    "6. **Appendix or Summary Section**\n",
    "   - Figure 9: Comprehensive Dashboard\n",
    "\n",
    "### Key Points to Highlight\n",
    "\n",
    "1. Dense BGE-M3 achieves highest MAP (0.546) - 108% improvement over BM25\n",
    "2. SPLADE significantly outperforms traditional BM25 (79% improvement)\n",
    "3. Hybrid methods show varying effectiveness depending on component combination\n",
    "4. All improvements are statistically significant (p < 0.001, large effect sizes)\n",
    "5. Trade-off between latency and quality exists (dense methods are slower)\n",
    "\n",
    "### LaTeX Integration\n",
    "\n",
    "```latex\n",
    "\\begin{figure}[htbp]\n",
    "    \\centering\n",
    "    \\includegraphics[width=0.9\\textwidth]{figures/fig1_overall_performance.pdf}\n",
    "    \\caption{Overall retrieval quality comparison across five methods. Dense BGE-M3 achieves \n",
    "    the highest performance across all metrics (MAP=0.546, MRR=0.928, NDCG@5=0.744), \n",
    "    representing a 108\\% improvement over the BM25 baseline. Error bars show Â±1 standard deviation.}\n",
    "    \\label{fig:exp1_overall}\n",
    "\\end{figure}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
